<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Karl Tayeb" />

<meta name="date" content="2022-10-20" />

<title>logistic_susie_initialization</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/main/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>



<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">logistic-susie-gsea</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/karltayeb/logistic-susie-gsea">
    <span class="fab fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">logistic_susie_initialization</h1>
<h4 class="author">Karl Tayeb</h4>
<h4 class="date">2022-10-20</h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span>
workflowr <span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2022-11-01
</p>
<p>
<strong>Checks:</strong> <span
class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7
<span class="glyphicon glyphicon-exclamation-sign text-danger"
aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>logistic-susie-gsea/</code> <span
class="glyphicon glyphicon-question-sign" aria-hidden="true"
title="This is the local directory in which the code in this file was executed.">
</span>
</p>
<p>
This reproducible <a href="https://rmarkdown.rstudio.com">R Markdown</a>
analysis was created with <a
  href="https://github.com/workflowr/workflowr">workflowr</a> (version
1.7.0). The <em>Checks</em> tab describes the reproducibility checks
that were applied when the results were created. The <em>Past
versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date
</a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git
repository, you know the exact version of the code that produced these
results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the
global environment can affect the analysis in your R Markdown file in
unknown ways. For reproduciblity it’s best to always run the code in an
empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20220105code">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Seed:</strong>
<code>set.seed(20220105)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20220105code"
class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20220105)</code> was run prior to running
the code in the R Markdown file. Setting a seed ensures that any results
that rely on randomness, e.g. subsampling or permutations, are
reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Session information:</strong>
recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded"
class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package
versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be
confident that you successfully produced the results during this
run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr
project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomkarltayeblogisticsusiegseatreea6493a8427caa489f8c9c406044c1eefe19979b5targetblanka6493a8a">
<span class="glyphicon glyphicon-ok text-success"
aria-hidden="true"></span> <strong>Repository version:</strong>
<a href="https://github.com/karltayeb/logistic-susie-gsea/tree/a6493a8427caa489f8c9c406044c1eefe19979b5" target="_blank">a6493a8</a>
</a>
</p>
</div>
<div
id="strongRepositoryversionstrongahrefhttpsgithubcomkarltayeblogisticsusiegseatreea6493a8427caa489f8c9c406044c1eefe19979b5targetblanka6493a8a"
class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development
and connecting the code version to the results is critical for
reproducibility.
</p>
<p>
The results in this page were generated with repository version
<a href="https://github.com/karltayeb/logistic-susie-gsea/tree/a6493a8427caa489f8c9c406044c1eefe19979b5" target="_blank">a6493a8</a>.
See the <em>Past versions</em> tab to see a history of the changes made
to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for
the analysis have been committed to Git prior to generating the results
(you can use <code>wflow_publish</code> or
<code>wflow_git_commit</code>). workflowr only checks the R Markdown
file, but you know if there are other scripts or data files that it
depends on. Below is the status of the Git repository when the results
were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .DS_Store
    Ignored:    .RData
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/
    Ignored:    _targets.R
    Ignored:    _targets.html
    Ignored:    _targets.md
    Ignored:    _targets/objects/
    Ignored:    _targets/user/
    Ignored:    _targets/workspaces/
    Ignored:    _targets_r/
    Ignored:    cache/
    Ignored:    data/.DS_Store
    Ignored:    data/adipose_2yr_topsnp.txt
    Ignored:    data/anthony/
    Ignored:    data/bohan_example/
    Ignored:    data/de-droplet/
    Ignored:    data/de-microplastics/
    Ignored:    data/deng/
    Ignored:    data/fetal_reference_cellid_gene_sets.RData
    Ignored:    data/human_chimp_eb/
    Ignored:    data/pbmc-purified/
    Ignored:    data/wenhe_baboon_diet/
    Ignored:    data/yusha_sc_tumor/
    Ignored:    library/
    Ignored:    renv/
    Ignored:    staging/

Untracked files:
    Untracked:  .ipynb_checkpoints/
    Untracked:  Master's Paper.pdf
    Untracked:  Project_1652928411/
    Untracked:  Project_1653228324/
    Untracked:  Project_1653228355/
    Untracked:  VEB_Boost_Proposal_Write_Up (1).pdf
    Untracked:  _targets/meta/
    Untracked:  additive.l5.gonr.aggregate.scores
    Untracked:  analysis/2022_09_22_pdac_example.Rmd
    Untracked:  analysis/alpha_ash_v_point_normal.Rmd
    Untracked:  analysis/bohan_troubleshoot.Rmd
    Untracked:  analysis/compare_w_post_hoc_clustering.Rmd
    Untracked:  analysis/compute_exact_BFs.Rmd
    Untracked:  analysis/de_droplet_noshrink.Rmd
    Untracked:  analysis/de_droplet_noshrink_logistic_susie.Rmd
    Untracked:  analysis/detection_problem.Rmd
    Untracked:  analysis/exact_logistic_ser.Rmd
    Untracked:  analysis/expected_condition_bfs.Rmd
    Untracked:  analysis/fetal_reference_cellid_gsea.Rmd
    Untracked:  analysis/fixed_intercept.Rmd
    Untracked:  analysis/gsea_made_simple.Rmd
    Untracked:  analysis/iDEA_examples.Rmd
    Untracked:  analysis/latent_gene_list.Rmd
    Untracked:  analysis/linear_method_failure_modes.Rmd
    Untracked:  analysis/linear_regression_failure_regime.Rmd
    Untracked:  analysis/linear_v_logistic_pbmc.Rmd
    Untracked:  analysis/logistic_susie_rss.Rmd
    Untracked:  analysis/logistic_susie_veb_boost_vs_vb.Rmd
    Untracked:  analysis/logistic_susie_vis.Rmd
    Untracked:  analysis/logistic_variational_bound.Rmd
    Untracked:  analysis/logsitic_susie_template.Rmd
    Untracked:  analysis/normal_means.Rmd
    Untracked:  analysis/pcb_scratch.Rmd
    Untracked:  analysis/references.bib
    Untracked:  analysis/roadmap.Rmd
    Untracked:  analysis/sc_tumor_followup.Rmd
    Untracked:  analysis/ser_detection_threshold.Rmd
    Untracked:  analysis/simulations.Rmd
    Untracked:  analysis/simulations_l1.Rmd
    Untracked:  analysis/tccm_vs_logistic_susie.Rmd
    Untracked:  analysis/template.Rmd
    Untracked:  analysis/test.Rmd
    Untracked:  analysis/univariate_laplace_approximation.Rmd
    Untracked:  analysis/vb_susie.Rmd
    Untracked:  analysis/z_score_vs_threshold.Rmd
    Untracked:  build_site.R
    Untracked:  code/binromial_ser.R
    Untracked:  code/html_tables.R
    Untracked:  code/latent_logistic_susie.R
    Untracked:  code/logistic_susie_data_driver.R
    Untracked:  code/marginal_sumstat_gsea_collapsed.R
    Untracked:  code/point_normal.R
    Untracked:  code/sumstat_gsea.py
    Untracked:  code/susie_gsea_queries.R
    Untracked:  docs.zip
    Untracked:  export/
    Untracked:  figure/
    Untracked:  l1.sim.aggregate.scores
    Untracked:  logistic_regression.stan
    Untracked:  pbmc_cd19_symbol.txt
    Untracked:  pbmc_cd19b_0.1_background.csv
    Untracked:  pbmc_cd19b_0.1_david_annotation_clusters.txt
    Untracked:  pbmc_cd19b_0.1_david_results.txt
    Untracked:  pbmc_cd19b_0.1_list.csv
    Untracked:  presentations/
    Untracked:  references.bib

Unstaged changes:
    Modified:   analysis/approximate_bayes_factors.Rmd
    Modified:   analysis/example_pbmc.Rmd

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not
included in this status report because it is ok for generated content to
have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were
made to the R Markdown
(<code>analysis/logistic_susie_initialization.Rmd</code>) and HTML
(<code>docs/logistic_susie_initialization.html</code>) files. If you’ve
configured a remote Git repository (see <code>?wflow_git_remote</code>),
click on the hyperlinks in the table below to view the files as they
were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/karltayeb/logistic-susie-gsea/blob/a6493a8427caa489f8c9c406044c1eefe19979b5/analysis/logistic_susie_initialization.Rmd" target="_blank">a6493a8</a>
</td>
<td>
Karl Tayeb
</td>
<td>
2022-11-01
</td>
<td>
wflow_publish("analysis/logistic_susie_initialization.Rmd")
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The purpose of this notebook is to explore the effect of
initialization on the results we get with the logistic SER and logistic
SuSiE.</p>
<p>First, We have real data examples from Yusha where the default
initialization does not result in a good final model fit. We will
explore step-wise regression based initialization and other options to
try and rescue this example.</p>
<p>Second, most of our simulations do not seem to struggle with the
default initialization, while the real data examples do. We should try
to reproduce the regimes that the default initialization fails.</p>
<p>Finally, even when we optimize to a good local optimum, we have
demonstrated the the credible sets from the logistic SER do not achieve
empirical coverage. In particular we have highlighted the role of the
Polya-Gamma variational parameters in complicating the fair comparison
of competing single effect models. The consequences are easy enough to
understand in the SER case, and we should also be cautious of it’s
impact on the behvior of logistic SuSiE.</p>
<pre class="r"><code>cache_rds &lt;- purrr::partial(xfun::cache_rds, dir=&#39;cache/initialization/&#39;)</code></pre>
</div>
<div id="yushas-examples" class="section level2">
<h2>Yusha’s Examples</h2>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──</code></pre>
<pre><code>✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
✔ tibble  3.1.8      ✔ dplyr   1.0.10
✔ tidyr   1.2.1      ✔ stringr 1.4.0 
✔ readr   2.1.2      ✔ forcats 0.5.2 </code></pre>
<pre><code>── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(tictoc)
library(kableExtra)</code></pre>
<pre><code>
Attaching package: &#39;kableExtra&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:dplyr&#39;:

    group_rows</code></pre>
<pre class="r"><code>devtools::load_all(&#39;~/R/logisticsusie/&#39;)</code></pre>
<pre><code>ℹ Loading logisticsusie</code></pre>
<pre class="r"><code>example &lt;- readRDS(&#39;data/yusha_sc_tumor/pdac_example.rds&#39;)
bindata &lt;- with(example, gseasusie::prep_binary_data(genesets, data, thresh = 0.01))
gs_names &lt;- colnames(bindata$X)
n_gene_sets &lt;- dim(bindata$X)[2]

example2 &lt;- readRDS(&#39;data/yusha_sc_tumor/pdac_example2.rds&#39;)
bindata2 &lt;- with(example, gseasusie::prep_binary_data(genesets, data, thresh = 0.01))
gs_names2 &lt;- colnames(bindata$X)
n_gene_sets2 &lt;- dim(bindata$X)[2]</code></pre>
<pre class="r"><code>get_elbo &lt;- function(fit){tail(fit$elbo, 1)}

set_xi &lt;- function(fit, xi){
  fit$params$xi &lt;- xi
  fit$params$tau &lt;- compute_tau(fit)
  return(fit)
}

get_bf &lt;- function(fit){
  compute_elbo2.binsusie(fit) - null_elbo
}

# compute lbf for each feature
get_lbf_variable &lt;- function(fit){
  p &lt;- dim(fit$data$X)[2]
  null_fit &lt;- fit_univariate_vb(fit$data$X[, 1], fit$data$y, tau0=1e10)
  lbf_variable &lt;- map_dbl(1:p, ~compute_elbo(
    x=fit$data$X[, .x],
    y=fit$data$y,
    o=0,
    mu=fit$params$mu[.x],
    tau=1/fit$params$var[.x],
    xi=fit$params$xi,
    delta=fit$params$delta[1,1],
    tau0=1/fit$hypers$prior_variance
  )) - tail(null_fit$elbos, 1)
  return(lbf_variable)
}

# compute lbf for the SER
get_lbf_ser &lt;- function(fit){
  null_fit &lt;- fit_univariate_vb(fit$data$X[, 1], fit$data$y, tau0=1e10)
  lbf &lt;- get_elbo(fit) - tail(null_fit$elbos, 1)
  return(lbf)
}</code></pre>
<div id="there-are-marginal-enrichments" class="section level3">
<h3>There are marginal enrichments</h3>
<p>We know the default initialization does not work well because
although there are several independent marginal association signals
logistic SuSiE does not identify any of them. There are several
independent enrichment signals, here we show the correlation among all
enrichments at a FDR of 5% (BH, Fisher’s exact test).</p>
<pre class="r"><code>ora &lt;- with(bindata, gseasusie::fit_ora(X, y))</code></pre>
<pre><code>computing ORA statistics...</code></pre>
<pre><code>0.66 sec elapsed</code></pre>
<pre class="r"><code>marginal_idx &lt;- which(p.adjust(ora$pFishersExact, method=&#39;BH&#39;) &lt; 0.05)
heatmap(cor(as.matrix(bindata$X[, marginal_idx])), scale=&#39;none&#39;, main=&#39;Marginally enriched gene sets, p.BH &lt; 0.05&#39;)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/marginal-results-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ora %&gt;% arrange(pFishersExact) %&gt;%
  mutate(nl10p_fishers = -log10(pFishersExact)) %&gt;%
  select(geneSet, geneSetSize, overlap, oddsRatio, nl10p_fishers) %&gt;%
  head(50) %&gt;%
  kbl(caption = &#39;Marginal enrichments ranked by -log10(p)&#39;) %&gt;% kable_styling()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
Marginal enrichments ranked by -log10(p)
</caption>
<thead>
<tr>
<th style="text-align:left;">
geneSet
</th>
<th style="text-align:right;">
geneSetSize
</th>
<th style="text-align:right;">
overlap
</th>
<th style="text-align:right;">
oddsRatio
</th>
<th style="text-align:right;">
nl10p_fishers
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
M522
</td>
<td style="text-align:right;">
75
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
6.4051846
</td>
<td style="text-align:right;">
5.652686
</td>
</tr>
<tr>
<td style="text-align:left;">
M5336
</td>
<td style="text-align:right;">
397
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
5.254835
</td>
</tr>
<tr>
<td style="text-align:left;">
M543
</td>
<td style="text-align:right;">
488
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.0599932
</td>
<td style="text-align:right;">
5.242160
</td>
</tr>
<tr>
<td style="text-align:left;">
M546
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
9.7949047
</td>
<td style="text-align:right;">
5.123475
</td>
</tr>
<tr>
<td style="text-align:left;">
M706
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
6.4035088
</td>
<td style="text-align:right;">
4.826582
</td>
</tr>
<tr>
<td style="text-align:left;">
M19248
</td>
<td style="text-align:right;">
53
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
6.7809197
</td>
<td style="text-align:right;">
4.579956
</td>
</tr>
<tr>
<td style="text-align:left;">
M5907
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
3.9036145
</td>
<td style="text-align:right;">
4.509714
</td>
</tr>
<tr>
<td style="text-align:left;">
M27416
</td>
<td style="text-align:right;">
47
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
6.7668593
</td>
<td style="text-align:right;">
4.133883
</td>
</tr>
<tr>
<td style="text-align:left;">
M27303
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
8.5311801
</td>
<td style="text-align:right;">
3.704613
</td>
</tr>
<tr>
<td style="text-align:left;">
M27372
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
18.5278450
</td>
<td style="text-align:right;">
3.629524
</td>
</tr>
<tr>
<td style="text-align:left;">
M27662
</td>
<td style="text-align:right;">
287
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
3.578823
</td>
</tr>
<tr>
<td style="text-align:left;">
M27410
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
16.2090395
</td>
<td style="text-align:right;">
3.463875
</td>
</tr>
<tr>
<td style="text-align:left;">
M27620
</td>
<td style="text-align:right;">
325
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
2.4855429
</td>
<td style="text-align:right;">
3.444443
</td>
</tr>
<tr>
<td style="text-align:left;">
M16843
</td>
<td style="text-align:right;">
470
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.1905180
</td>
<td style="text-align:right;">
3.412726
</td>
</tr>
<tr>
<td style="text-align:left;">
M27958
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
7.2622222
</td>
<td style="text-align:right;">
3.381053
</td>
</tr>
<tr>
<td style="text-align:left;">
M27412
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
14.4055242
</td>
<td style="text-align:right;">
3.314606
</td>
</tr>
<tr>
<td style="text-align:left;">
M653
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
12.9627119
</td>
<td style="text-align:right;">
3.178900
</td>
</tr>
<tr>
<td style="text-align:left;">
M27483
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
11.7822291
</td>
<td style="text-align:right;">
3.054613
</td>
</tr>
<tr>
<td style="text-align:left;">
M556
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
11.7822291
</td>
<td style="text-align:right;">
3.054613
</td>
</tr>
<tr>
<td style="text-align:left;">
M29844
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
24.1938202
</td>
<td style="text-align:right;">
3.047924
</td>
</tr>
<tr>
<td style="text-align:left;">
M5946
</td>
<td style="text-align:right;">
84
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
3.9565116
</td>
<td style="text-align:right;">
3.013947
</td>
</tr>
<tr>
<td style="text-align:left;">
M41835
</td>
<td style="text-align:right;">
72
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
4.1054913
</td>
<td style="text-align:right;">
2.835828
</td>
</tr>
<tr>
<td style="text-align:left;">
M820
</td>
<td style="text-align:right;">
29
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6.7732008
</td>
<td style="text-align:right;">
2.784119
</td>
</tr>
<tr>
<td style="text-align:left;">
M13087
</td>
<td style="text-align:right;">
213
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.714742
</td>
</tr>
<tr>
<td style="text-align:left;">
M495
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6.2500000
</td>
<td style="text-align:right;">
2.649990
</td>
</tr>
<tr>
<td style="text-align:left;">
M649
</td>
<td style="text-align:right;">
116
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
3.1115525
</td>
<td style="text-align:right;">
2.565117
</td>
</tr>
<tr>
<td style="text-align:left;">
M684
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
8.0932203
</td>
<td style="text-align:right;">
2.556361
</td>
</tr>
<tr>
<td style="text-align:left;">
M3634
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
13.8178170
</td>
<td style="text-align:right;">
2.542294
</td>
</tr>
<tr>
<td style="text-align:left;">
M15434
</td>
<td style="text-align:right;">
201
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.536525
</td>
</tr>
<tr>
<td style="text-align:left;">
M27334
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5.8015422
</td>
<td style="text-align:right;">
2.526259
</td>
</tr>
<tr>
<td style="text-align:left;">
M1070
</td>
<td style="text-align:right;">
208
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.519135
</td>
</tr>
<tr>
<td style="text-align:left;">
M1036
</td>
<td style="text-align:right;">
630
</td>
<td style="text-align:right;">
32
</td>
<td style="text-align:right;">
1.8488474
</td>
<td style="text-align:right;">
2.502054
</td>
</tr>
<tr>
<td style="text-align:left;">
M734
</td>
<td style="text-align:right;">
768
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.4264434
</td>
<td style="text-align:right;">
2.480535
</td>
</tr>
<tr>
<td style="text-align:left;">
M525
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
12.0884831
</td>
<td style="text-align:right;">
2.413791
</td>
</tr>
<tr>
<td style="text-align:left;">
M18647
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
12.0884831
</td>
<td style="text-align:right;">
2.413791
</td>
</tr>
<tr>
<td style="text-align:left;">
M41824
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
7.1914626
</td>
<td style="text-align:right;">
2.398132
</td>
</tr>
<tr>
<td style="text-align:left;">
M27185
</td>
<td style="text-align:right;">
189
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.358309
</td>
</tr>
<tr>
<td style="text-align:left;">
M11575
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
10.7434457
</td>
<td style="text-align:right;">
2.298636
</td>
</tr>
<tr>
<td style="text-align:left;">
M2341
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4.9182163
</td>
<td style="text-align:right;">
2.254359
</td>
</tr>
<tr>
<td style="text-align:left;">
M11980
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
9.6674157
</td>
<td style="text-align:right;">
2.194462
</td>
</tr>
<tr>
<td style="text-align:left;">
M971
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
5.8798151
</td>
<td style="text-align:right;">
2.128963
</td>
</tr>
<tr>
<td style="text-align:left;">
M913
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
8.7870276
</td>
<td style="text-align:right;">
2.099481
</td>
</tr>
<tr>
<td style="text-align:left;">
M27275
</td>
<td style="text-align:right;">
76
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
3.3099284
</td>
<td style="text-align:right;">
2.087942
</td>
</tr>
<tr>
<td style="text-align:left;">
M27103
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4.3834459
</td>
<td style="text-align:right;">
2.067821
</td>
</tr>
<tr>
<td style="text-align:left;">
M5485
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
3.6139683
</td>
<td style="text-align:right;">
2.021190
</td>
</tr>
<tr>
<td style="text-align:left;">
M5901
</td>
<td style="text-align:right;">
163
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.003710
</td>
</tr>
<tr>
<td style="text-align:left;">
M27050
</td>
<td style="text-align:right;">
164
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
2.003024
</td>
</tr>
<tr>
<td style="text-align:left;">
M5925
</td>
<td style="text-align:right;">
172
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
1.976110
</td>
</tr>
<tr>
<td style="text-align:left;">
M14033
</td>
<td style="text-align:right;">
172
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.0000000
</td>
<td style="text-align:right;">
1.976110
</td>
</tr>
<tr>
<td style="text-align:left;">
M5930
</td>
<td style="text-align:right;">
141
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
2.5065845
</td>
<td style="text-align:right;">
1.970215
</td>
</tr>
</tbody>
</table>
</div>
<div id="default-initialization-converges-to-the-null-model-or-worse"
class="section level3">
<h3>Default initialization converges to the null model (or worse)</h3>
<p>Unfortunately, our default initialization yields NO significant
enrichment for either logistic SuSiE or the logistic SER. If we fix the
prior variance, we learn a model that has a worse ELBO than the null
model. If we estimate the prior variance parametes, they are shrunk
close to zero (i.e. we estimate the null model).</p>
<pre class="r"><code>tic(&#39;L=10, fixed prior variance&#39;)
fit.default &lt;- cache_rds(with(bindata, binsusie(X, y, prior_variance=10, estimate_prior_variance=F)), file=&#39;fit.default&#39;)
toc()</code></pre>
<pre><code>L=10, fixed prior variance: 0.635 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(fit.default)</code></pre>
<pre><code>[1] -821.4436</code></pre>
<pre class="r"><code>tic(&#39;L=10, estimate prior variance&#39;)
fit.default2 &lt;- cache_rds(with(bindata, binsusie(X, y, prior_variance=10, estimate_prior_variance=T)), file=&#39;fit.default2&#39;)
toc()</code></pre>
<pre><code>L=10, estimate prior variance: 0.567 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(fit.default2)</code></pre>
<pre><code>[1] -809.7458</code></pre>
<pre class="r"><code>tic(&#39;L=10, null model&#39;)
fit.null &lt;- cache_rds(with(bindata, binsusie(X, y, prior_variance=1e-10, estimate_prior_variance=F)), file=&#39;fit.null&#39;)
toc()</code></pre>
<pre><code>L=10, null model: 0.442 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(fit.null)</code></pre>
<pre><code>[1] -809.6773</code></pre>
<pre class="r"><code>tic(&#39;SER, fixed prior variance&#39;)
ser.default &lt;- cache_rds(with(bindata, binsusie(X, y, L = 1, prior_variance = 10, estimate_prior_variance = F)), file=&#39;ser.default&#39;)
toc()</code></pre>
<pre><code>SER, fixed prior variance: 0.441 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(ser.default)</code></pre>
<pre><code>[1] -810.7019</code></pre>
<pre class="r"><code>tic(&#39;SER, estimate prior variance&#39;)
ser.default2 &lt;- cache_rds(with(bindata, binsusie(X, y, L = 1, prior_variance = 10, estimate_prior_variance = T)), file=&#39;ser.default2&#39;)
toc()</code></pre>
<pre><code>SER, estimate prior variance: 0.24 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(ser.default2)</code></pre>
<pre><code>[1] -809.697</code></pre>
<pre class="r"><code>tic(&#39;SER, null model&#39;)
ser.null &lt;- cache_rds(with(bindata, binsusie(X, y, L = 1, prior_variance = 1e-10, estimate_prior_variance = T)), file=&#39;ser.null&#39;)
toc()</code></pre>
<pre><code>SER, null model: 0.429 sec elapsed</code></pre>
<pre class="r"><code>get_elbo(ser.null)</code></pre>
<pre><code>[1] -809.6773</code></pre>
<pre class="r"><code>get_all_cs(fit.default)$L1</code></pre>
<pre><code>$cs
   [1] 1303  773  607  193  102  479  291 1180  288  200  184 1151  363  108
  [15]  315  163  286  829 1203 1218 1195  772  396  296  469   97   78   16
  [29] 1181   87   91   85  917  707  723 1053  194  668  810  554  358  870
  [43]  126  896 1282  470  662   84   52 1039  729  645 1212  591  347  461
  [57]  616  738  780  555  471  357  154  266  915  218  774  445 1312  468
  [71]   83  691  899 1045 1326   10  129 1215 1020 1026  331  309  788  801
  [85]  156  622  405  703 1321  795  186 1050  978   89   86  537 1294  127
  [99]  253  572 1217   93 1182  256  413  406  408  592  808  310  818  769
 [113]  945   55 1134 1270  809  781  782  111  792  954  105  172  502  217
 [127]   80  229 1044  834 1052  273  850 1191  814  852  979  415  826  902
 [141]  728  414  132   63  620  458 1025 1311  379  973  970  117  980  455
 [155]  614  400  920  650 1210  433  815  923  750   60  410  412  570 1329
 [169] 1306 1179  710  531 1141 1327  155   75 1209 1288  254  813 1188 1302
 [183]  360  474  832 1139   90  491  135  189  588  881  306 1317  387  618
 [197]  374  718 1200  573  575   59 1055  174   82  380 1319 1235  305 1193
 [211]  403  901  124  720   77  274  664  235  853  295  882  182  647  606
 [225]  386  708 1318  529  906  293 1064 1249 1324  971  532  640  372  830
 [239]  594   62 1223  628  375 1241  743  456 1206  873  191  137 1168  180
 [253]  261  103 1185  603 1172  696  990  908  883  946  858  511 1229 1287
 [267]  857  168  162 1051 1251  443 1293  268  278  976  631  325 1147  736
 [281]  354  701  370  567 1038  833  784 1330  735  258    6 1325  697  125
 [295]  593 1221  912 1061  104 1247  820  715  819  821  601  897  825   76
 [309] 1148  579  868 1101 1205  419  643  100  747  336  227 1216  910  595
 [323]  587  271  510  968  134  879  299 1177  542  202 1063 1176  930  674
 [337]  771  442   94  887 1056 1057 1090  933  337  734 1245  494 1219  822
 [351]  807  791  482  495 1255  619 1283  365 1158 1065  733 1213  787 1173
 [365]  911  505  859  149  230  597  141  418 1301  314  837 1187  475  257
 [379]  665  478  160 1320  700  279   92  150  109  435  434  436 1058 1092
 [393] 1118  641  244  381  755 1165  483   95  981 1295  799  578  540 1033
 [407] 1019  630  267  921 1254  950  476  918 1310   96  919 1243  634  534
 [421] 1192  423  361  383  900  955 1238  151  101 1224 1184  528 1166  766
 [435]  880  651 1208  580  982  732  811  277  158  437 1222  816  463  544
 [449]   71 1059 1107  136  928  789 1032  842  181 1156 1227  480 1274  238
 [463]  854 1308 1150 1135  142  644 1183  290 1190 1323  675  800  444   72
 [477]  658  313  211  441   68  653  255  793 1018  453  856  877  841  866
 [491]  843  378  869 1104  927  751  926 1067 1244  649  839  835  239  798
 [505]  686  352  939  805  213  487  225  669  260  242 1113 1078  176 1060
 [519] 1220  600  454  663  632  602  294  737  571   65  670  489 1000 1246
 [533]  576   99 1250  846   57 1174  303  524  878  690  678  864  581  173
 [547]  276  344  345 1144  590 1094  530   64  625 1313  931 1046  865  133
 [561] 1062  153 1073  749  431   56  783 1163  245  430  627   38  287  548
 [575]  420  656  689 1322  705  609   73  346  343  558 1277 1278 1133  617
 [589]  496 1031  377  760  828  128  246  302  965  199  448  428 1043 1178
 [603]  130  175  699 1201  376 1070  523  964  165  170 1068  114  318  243
 [617]  929  969  898 1117  612  717  951  394  553 1269  549  188  746 1328
 [631]  817  731  432 1010  460 1240  167 1296  863  356 1152  770  106  234
 [645] 1100 1081 1079 1112  621  399  827 1074  223  611  350  539  844  941
 [659] 1242  183  446 1041  450  541  466  960   51 1097  547  503  712  711
 [673]  713  210  944  983  533  338  209  604  262  481  185 1030  764  895
 [687] 1268 1248 1021  319  726 1159  457 1234 1066  998  714  113  966  119
 [701]  748  838  411 1087  796  328  636  543 1047 1171  959  236  283 1297
 [715] 1272  672 1093 1022 1076 1252  275  316 1233  404  263 1006  884  637
 [729]  695  304  397  221  488  212  311  913 1194  459  385   61  122  847
 [743]  958  988 1013 1014  687  999  407  166 1309  241  247  849   58  744
 [757]  667 1003  164  317  364 1291  673  535 1298  777  462 1072  195  739
 [771]  355  679  922  989  425  447  140  885  767  472  143 1226  340  362
 [785]   20  197 1199  147  368 1028 1012 1002  177 1001  947 1265  934 1145
 [799]  741  159  341  599  861   79 1089  366  477 1091   98 1204  613  292
 [813]  339  389  688  312  515  633  359 1259  179  493  624  639  598 1138
 [827]  903 1131  761  702  409  506  949  157 1109   81  514 1088 1085 1132
 [841]   35  569  232  894  416 1275 1170 1161   49  335 1128  321  546  501
 [855]  804 1084  574   42  704   69  417  874  762 1290  753  427   67  742
 [869] 1103  577 1314  725 1279 1169 1207  556  557  957  995  520    2  214
 [883]  464  138  525 1121  564  953    4  775  722 1267 1164  756  187  585
 [897] 1110  330   15 1027  486  694 1142 1080  987  148  615  518  388 1077
 [911] 1011  265  583  759   14  327  785  566  709  716 1196  107  907  642
 [925]  348  512 1285  608  806  526  249  390  752   70 1160 1162 1005  519
 [939]  840  765  551  745  984  421 1260 1316 1257 1075  763 1149  845 1236
 [953]  635  527  326  565 1186 1004 1157  281 1258    3 1264  956 1300  889
 [967]  429  259  550 1140  500  972  996 1108  831 1124  886  504 1049  424
 [981]  757 1116 1154  909  112 1225   74  422 1237 1083  369 1048  171  300
 [995]  282 1023  297  660  802  139 1189  997  914  803  693  862  812  206
[1009] 1146  322 1016 1017  623  671  272  855  943 1202   43 1040  192 1086
[1023]  351  507 1007  993  231  610  349  285 1256  280  498 1106   33  629
[1037] 1276  240 1029  289  754 1015  924  986 1042  251  823  449  384  758
[1051]  220  233   54  439  145  786  940  942 1082  938 1214  991  509  778
[1065]  538  353  161 1009  120  391 1119 1155  205 1284 1253  497   28  264
[1079]  110  401 1035  904   66  596 1299  916 1271  371  730  905  559  342
[1093]  248 1228  952  216  888  936  794  684  298  586  467  237 1286  499
[1107]  521  962 1071 1069  334  121 1167  992 1305  740  552  661 1098  252
[1121]  438 1263  228  963  563  426  605 1127  568  440 1198  719

$prob
   [1] 0.0170497468 0.0167693623 0.0166800053 0.0120256698 0.0099116971
   [6] 0.0092287215 0.0091237282 0.0091090659 0.0067782340 0.0064587150
  [11] 0.0062168825 0.0053925473 0.0053471822 0.0053119270 0.0049679753
  [16] 0.0046413570 0.0045409838 0.0043317180 0.0042307510 0.0041692218
  [21] 0.0041097138 0.0040808583 0.0040740089 0.0037917458 0.0035256421
  [26] 0.0035009395 0.0032822505 0.0032314528 0.0032174451 0.0029476111
  [31] 0.0029126513 0.0028913506 0.0026856262 0.0026033976 0.0025769431
  [36] 0.0025147446 0.0025095700 0.0024758164 0.0024593239 0.0024515539
  [41] 0.0024471964 0.0022817572 0.0022797679 0.0021207030 0.0020976157
  [46] 0.0018636361 0.0018574592 0.0018561216 0.0018478812 0.0018418062
  [51] 0.0018262949 0.0018061198 0.0017688131 0.0017644453 0.0017187087
  [56] 0.0017155444 0.0016930199 0.0016913323 0.0016913152 0.0016911222
  [61] 0.0016909239 0.0016902168 0.0016872406 0.0016871006 0.0016868806
  [66] 0.0016847411 0.0016728887 0.0015862678 0.0015756015 0.0015753912
  [71] 0.0015350896 0.0014711519 0.0014574896 0.0014538875 0.0014477826
  [76] 0.0014293001 0.0014095274 0.0013635665 0.0013543546 0.0013529237
  [81] 0.0013523694 0.0013523009 0.0013521612 0.0013514855 0.0013514075
  [86] 0.0013512918 0.0013509799 0.0013501437 0.0013501199 0.0013498858
  [91] 0.0013468852 0.0013458873 0.0013435759 0.0013403769 0.0013341361
  [96] 0.0013199337 0.0013032664 0.0012966941 0.0012341956 0.0012298786
 [101] 0.0012112413 0.0011969897 0.0011923460 0.0011770426 0.0011696242
 [106] 0.0011692982 0.0011692982 0.0011632840 0.0011624216 0.0011612406
 [111] 0.0011610931 0.0011609147 0.0011608282 0.0011606040 0.0011606009
 [116] 0.0011605942 0.0011604078 0.0011603120 0.0011603120 0.0011602751
 [121] 0.0011602126 0.0011600461 0.0011598705 0.0011594744 0.0011591123
 [126] 0.0011588969 0.0011527097 0.0011468254 0.0011283604 0.0011060113
 [131] 0.0010903992 0.0010780965 0.0010729598 0.0010671738 0.0010425932
 [136] 0.0010381297 0.0010381053 0.0010373090 0.0010367723 0.0010361902
 [141] 0.0010361410 0.0010358695 0.0010346897 0.0010343977 0.0010343296
 [146] 0.0010343073 0.0010342807 0.0010336926 0.0010336630 0.0010335573
 [151] 0.0010317689 0.0010106840 0.0009847093 0.0009714401 0.0009655106
 [156] 0.0009595832 0.0009568258 0.0009531285 0.0009521744 0.0009497395
 [161] 0.0009495881 0.0009493727 0.0009490795 0.0009486253 0.0009486179
 [166] 0.0009486179 0.0009478846 0.0009476476 0.0009458895 0.0009458410
 [171] 0.0009445341 0.0009438843 0.0009438195 0.0009437119 0.0009433829
 [176] 0.0009433184 0.0009429246 0.0009428829 0.0009428463 0.0009427171
 [181] 0.0009426471 0.0009424591 0.0009424262 0.0009424098 0.0009413965
 [186] 0.0009407546 0.0009402480 0.0009386550 0.0009374762 0.0009127897
 [191] 0.0008913963 0.0008869956 0.0008863457 0.0008786463 0.0008768738
 [196] 0.0008765461 0.0008758055 0.0008757620 0.0008754593 0.0008753834
 [201] 0.0008753834 0.0008752456 0.0008752456 0.0008749915 0.0008748547
 [206] 0.0008745084 0.0008740598 0.0008739201 0.0008735761 0.0008734821
 [211] 0.0008733708 0.0008733544 0.0008732748 0.0008732479 0.0008732163
 [216] 0.0008731798 0.0008731767 0.0008731672 0.0008731609 0.0008731558
 [221] 0.0008731392 0.0008728718 0.0008727100 0.0008726760 0.0008726036
 [226] 0.0008711443 0.0008710405 0.0008708122 0.0008701445 0.0008683092
 [231] 0.0008637133 0.0008612634 0.0008600057 0.0008599167 0.0008543144
 [236] 0.0008465449 0.0008352709 0.0008337668 0.0008275967 0.0008273582
 [241] 0.0008226287 0.0008210218 0.0008198471 0.0008196274 0.0008194399
 [246] 0.0008190584 0.0008188208 0.0008186403 0.0008185983 0.0008185818
 [251] 0.0008183570 0.0008181229 0.0008180827 0.0008178018 0.0008177597
 [256] 0.0008177556 0.0008177443 0.0008177404 0.0008176870 0.0008176609
 [261] 0.0008176544 0.0008175556 0.0008175247 0.0008173957 0.0008173822
 [266] 0.0008173295 0.0008173052 0.0008172871 0.0008171707 0.0008169750
 [271] 0.0008167776 0.0008158817 0.0008148588 0.0008132378 0.0008131257
 [276] 0.0008125083 0.0008024972 0.0008022982 0.0008019943 0.0008000821
 [281] 0.0007991096 0.0007983820 0.0007982740 0.0007972406 0.0007970998
 [286] 0.0007958952 0.0007951567 0.0007938178 0.0007936836 0.0007935503
 [291] 0.0007931320 0.0007925780 0.0007865905 0.0007829885 0.0007811004
 [296] 0.0007809965 0.0007808068 0.0007807499 0.0007807050 0.0007806151
 [301] 0.0007802432 0.0007801760 0.0007801420 0.0007799097 0.0007798484
 [306] 0.0007785193 0.0007770075 0.0007769939 0.0007755413 0.0007753684
 [311] 0.0007746560 0.0007744476 0.0007741399 0.0007736976 0.0007735272
 [316] 0.0007734741 0.0007734378 0.0007732163 0.0007729188 0.0007728794
 [321] 0.0007725150 0.0007722968 0.0007722522 0.0007720362 0.0007720201
 [326] 0.0007719517 0.0007719102 0.0007718928 0.0007718595 0.0007718138
 [331] 0.0007717149 0.0007716926 0.0007716109 0.0007715668 0.0007712186
 [336] 0.0007711391 0.0007694912 0.0007641599 0.0007581448 0.0007460691
 [341] 0.0007439271 0.0007437250 0.0007425766 0.0007424371 0.0007422405
 [346] 0.0007420662 0.0007419497 0.0007417673 0.0007414627 0.0007413810
 [351] 0.0007412391 0.0007411671 0.0007407360 0.0007373602 0.0007360576
 [356] 0.0007360165 0.0007357903 0.0007356579 0.0007353427 0.0007353324
 [361] 0.0007352098 0.0007350020 0.0007347710 0.0007347214 0.0007347211
 [366] 0.0007346509 0.0007346340 0.0007346125 0.0007345001 0.0007343323
 [371] 0.0007339212 0.0007338938 0.0007337906 0.0007337562 0.0007337388
 [376] 0.0007337380 0.0007336480 0.0007336260 0.0007335419 0.0007335347
 [381] 0.0007333837 0.0007333253 0.0007238017 0.0007194161 0.0007181945
 [386] 0.0007142425 0.0007119704 0.0007113188 0.0007112410 0.0007110636
 [391] 0.0007104320 0.0007100664 0.0007095748 0.0007091553 0.0007084645
 [396] 0.0007083833 0.0007078944 0.0007077959 0.0007074767 0.0007072691
 [401] 0.0007062898 0.0007045048 0.0007044045 0.0007041296 0.0007040201
 [406] 0.0007037206 0.0007035654 0.0007034215 0.0007032843 0.0007031681
 [411] 0.0007030751 0.0007026301 0.0007026120 0.0007024119 0.0007024029
 [416] 0.0007023843 0.0007023720 0.0007023181 0.0007021903 0.0007020827
 [421] 0.0007020827 0.0007020369 0.0007019960 0.0007017666 0.0007017612
 [426] 0.0007017561 0.0007017060 0.0007014908 0.0007014507 0.0007013098
 [431] 0.0007012367 0.0007011923 0.0007011602 0.0007010566 0.0007009612
 [436] 0.0007008382 0.0006992575 0.0006976862 0.0006954144 0.0006950546
 [441] 0.0006934279 0.0006898915 0.0006837511 0.0006822394 0.0006814793
 [446] 0.0006811209 0.0006808556 0.0006805434 0.0006802310 0.0006800567
 [451] 0.0006790039 0.0006789921 0.0006789845 0.0006772237 0.0006749440
 [456] 0.0006744587 0.0006741764 0.0006739543 0.0006737274 0.0006736796
 [461] 0.0006736009 0.0006735897 0.0006734408 0.0006733906 0.0006733855
 [466] 0.0006733268 0.0006731313 0.0006730409 0.0006730392 0.0006730197
 [471] 0.0006728510 0.0006726877 0.0006726666 0.0006726457 0.0006719097
 [476] 0.0006707550 0.0006632523 0.0006630026 0.0006627696 0.0006618925
 [481] 0.0006615058 0.0006594225 0.0006546713 0.0006546451 0.0006541817
 [486] 0.0006535604 0.0006530587 0.0006521001 0.0006513697 0.0006510652
 [491] 0.0006509430 0.0006509331 0.0006507351 0.0006503523 0.0006502133
 [496] 0.0006501069 0.0006500991 0.0006500022 0.0006498155 0.0006498145
 [501] 0.0006492183 0.0006491026 0.0006490838 0.0006490670 0.0006488462
 [506] 0.0006486880 0.0006484463 0.0006483680 0.0006483395 0.0006483278
 [511] 0.0006481015 0.0006480123 0.0006479403 0.0006346501 0.0006346077
 [516] 0.0006346036 0.0006330821 0.0006330227 0.0006326504 0.0006323602
 [521] 0.0006322242 0.0006321432 0.0006303135 0.0006293278 0.0006290448
 [526] 0.0006289518 0.0006288165 0.0006287638 0.0006283419 0.0006283141
 [531] 0.0006279137 0.0006279042 0.0006278954 0.0006278257 0.0006277223
 [536] 0.0006276417 0.0006275701 0.0006273969 0.0006273778 0.0006267069
 [541] 0.0006262253 0.0006261644 0.0006260668 0.0006260598 0.0006226647
 [546] 0.0006226428 0.0006190953 0.0006171286 0.0006169585 0.0006168928
 [551] 0.0006162109 0.0006143507 0.0006127955 0.0006127614 0.0006125397
 [556] 0.0006118778 0.0006114291 0.0006105798 0.0006094256 0.0006092674
 [561] 0.0006091633 0.0006088147 0.0006088147 0.0006087008 0.0006082900
 [566] 0.0006081981 0.0006081782 0.0006080976 0.0006080899 0.0006078075
 [571] 0.0006077905 0.0006073960 0.0006073828 0.0006073435 0.0006070526
 [576] 0.0006069757 0.0006069036 0.0006067724 0.0006066271 0.0006012797
 [581] 0.0005992568 0.0005990196 0.0005990011 0.0005960496 0.0005955527
 [586] 0.0005954430 0.0005952118 0.0005934066 0.0005926522 0.0005921132
 [591] 0.0005920416 0.0005918485 0.0005915741 0.0005915402 0.0005914924
 [596] 0.0005914822 0.0005910951 0.0005910811 0.0005910785 0.0005907916
 [601] 0.0005905581 0.0005903685 0.0005902902 0.0005902595 0.0005901876
 [606] 0.0005900004 0.0005899226 0.0005895059 0.0005894887 0.0005893722
 [611] 0.0005890369 0.0005878964 0.0005862086 0.0005854332 0.0005853042
 [616] 0.0005848423 0.0005815835 0.0005811308 0.0005802271 0.0005796251
 [621] 0.0005790247 0.0005789192 0.0005782587 0.0005779706 0.0005778287
 [626] 0.0005775448 0.0005773889 0.0005763570 0.0005753470 0.0005749351
 [631] 0.0005749143 0.0005747426 0.0005745146 0.0005744826 0.0005743566
 [636] 0.0005740171 0.0005739888 0.0005736995 0.0005734724 0.0005734137
 [641] 0.0005733561 0.0005732896 0.0005731876 0.0005697498 0.0005674883
 [646] 0.0005665185 0.0005663971 0.0005654901 0.0005640872 0.0005634697
 [651] 0.0005627238 0.0005624509 0.0005624288 0.0005614976 0.0005612195
 [656] 0.0005611084 0.0005610332 0.0005608692 0.0005606713 0.0005603834
 [661] 0.0005603606 0.0005602600 0.0005602475 0.0005598850 0.0005598270
 [666] 0.0005597282 0.0005594821 0.0005594494 0.0005593626 0.0005585846
 [671] 0.0005562020 0.0005561629 0.0005560755 0.0005556212 0.0005541193
 [676] 0.0005529927 0.0005518695 0.0005512256 0.0005506131 0.0005502655
 [681] 0.0005502517 0.0005499679 0.0005490611 0.0005483557 0.0005482455
 [686] 0.0005481761 0.0005477813 0.0005471379 0.0005470589 0.0005470343
 [691] 0.0005468759 0.0005463794 0.0005460516 0.0005457164 0.0005452712
 [696] 0.0005450520 0.0005435983 0.0005388251 0.0005383094 0.0005381760
 [701] 0.0005380505 0.0005379386 0.0005369204 0.0005369204 0.0005356299
 [706] 0.0005349142 0.0005347428 0.0005346261 0.0005344822 0.0005342884
 [711] 0.0005341741 0.0005334725 0.0005333230 0.0005332900 0.0005332678
 [716] 0.0005331723 0.0005330675 0.0005326360 0.0005325435 0.0005296068
 [721] 0.0005268279 0.0005257559 0.0005252506 0.0005243640 0.0005238655
 [726] 0.0005235489 0.0005233717 0.0005230992 0.0005223934 0.0005222897
 [731] 0.0005221272 0.0005219305 0.0005209387 0.0005190125 0.0005182690
 [736] 0.0005181103 0.0005170186 0.0005169664 0.0005164272 0.0005161781
 [741] 0.0005158412 0.0005158185 0.0005138168 0.0005126100 0.0005125677
 [746] 0.0005125519 0.0005118500 0.0005079711 0.0005068045 0.0005054817
 [751] 0.0005045779 0.0005040406 0.0005038922 0.0005038711 0.0005037606
 [756] 0.0005037248 0.0005035002 0.0005033404 0.0005029187 0.0005027801
 [761] 0.0005022698 0.0005021617 0.0005020749 0.0005019535 0.0005017479
 [766] 0.0005015961 0.0005011673 0.0005010922 0.0005006649 0.0004983085
 [771] 0.0004981222 0.0004975812 0.0004950332 0.0004948512 0.0004945585
 [776] 0.0004935422 0.0004934544 0.0004932461 0.0004931319 0.0004930550
 [781] 0.0004886541 0.0004886179 0.0004879011 0.0004878544 0.0004878263
 [786] 0.0004875772 0.0004869277 0.0004868507 0.0004859435 0.0004858544
 [791] 0.0004853375 0.0004847706 0.0004846849 0.0004808663 0.0004807488
 [796] 0.0004789398 0.0004788924 0.0004778540 0.0004774310 0.0004773542
 [801] 0.0004772364 0.0004771574 0.0004769208 0.0004767043 0.0004764268
 [806] 0.0004757149 0.0004749023 0.0004748403 0.0004743090 0.0004737464
 [811] 0.0004734497 0.0004732297 0.0004729884 0.0004711293 0.0004710322
 [816] 0.0004706809 0.0004706686 0.0004705964 0.0004699103 0.0004698832
 [821] 0.0004696263 0.0004692376 0.0004685350 0.0004675254 0.0004670645
 [826] 0.0004668248 0.0004666111 0.0004642488 0.0004641812 0.0004639226
 [831] 0.0004638573 0.0004638520 0.0004638113 0.0004636914 0.0004636593
 [836] 0.0004636470 0.0004633701 0.0004630198 0.0004626268 0.0004621520
 [841] 0.0004617840 0.0004613852 0.0004611917 0.0004604388 0.0004595836
 [846] 0.0004593409 0.0004586586 0.0004578021 0.0004577632 0.0004576205
 [851] 0.0004573484 0.0004571347 0.0004570318 0.0004564974 0.0004562826
 [856] 0.0004555779 0.0004550556 0.0004546464 0.0004538533 0.0004531713
 [861] 0.0004527273 0.0004514622 0.0004514299 0.0004512936 0.0004512152
 [866] 0.0004511920 0.0004509824 0.0004509190 0.0004508845 0.0004505719
 [871] 0.0004480692 0.0004479083 0.0004477654 0.0004475965 0.0004470883
 [876] 0.0004454026 0.0004453635 0.0004453154 0.0004451693 0.0004451412
 [881] 0.0004448443 0.0004448104 0.0004447165 0.0004443574 0.0004436263
 [886] 0.0004430132 0.0004430000 0.0004426757 0.0004423053 0.0004422965
 [891] 0.0004412486 0.0004401979 0.0004400334 0.0004397990 0.0004394719
 [896] 0.0004389346 0.0004379337 0.0004362331 0.0004356409 0.0004344197
 [901] 0.0004338757 0.0004337664 0.0004327472 0.0004323509 0.0004308487
 [906] 0.0004296423 0.0004294265 0.0004293556 0.0004287093 0.0004286025
 [911] 0.0004284854 0.0004281521 0.0004279958 0.0004275345 0.0004270964
 [916] 0.0004264526 0.0004262070 0.0004246003 0.0004245153 0.0004243962
 [921] 0.0004235751 0.0004235427 0.0004226086 0.0004224832 0.0004223566
 [926] 0.0004207212 0.0004200524 0.0004194123 0.0004176711 0.0004172382
 [931] 0.0004169786 0.0004161207 0.0004160763 0.0004159063 0.0004158994
 [936] 0.0004157973 0.0004156358 0.0004154354 0.0004151636 0.0004146119
 [941] 0.0004143226 0.0004142964 0.0004140598 0.0004134875 0.0004125540
 [946] 0.0004123132 0.0004121288 0.0004114642 0.0004098337 0.0004092896
 [951] 0.0004086273 0.0004077535 0.0004077186 0.0004075574 0.0004059381
 [956] 0.0004044634 0.0004043534 0.0004041862 0.0004041330 0.0004030618
 [961] 0.0004029192 0.0004019162 0.0004008631 0.0004006117 0.0003999603
 [966] 0.0003989677 0.0003984612 0.0003973976 0.0003968143 0.0003966032
 [971] 0.0003933755 0.0003921201 0.0003913249 0.0003905732 0.0003899555
 [976] 0.0003897276 0.0003893575 0.0003893224 0.0003892139 0.0003892004
 [981] 0.0003884100 0.0003878741 0.0003875204 0.0003872055 0.0003861886
 [986] 0.0003849467 0.0003841232 0.0003840679 0.0003840169 0.0003839709
 [991] 0.0003836921 0.0003833720 0.0003825284 0.0003823085 0.0003804731
 [996] 0.0003800384 0.0003784021 0.0003780871 0.0003780453 0.0003777780
[1001] 0.0003774159 0.0003768313 0.0003765952 0.0003765481 0.0003756360
[1006] 0.0003756233 0.0003746767 0.0003737970 0.0003735906 0.0003733133
[1011] 0.0003726681 0.0003724895 0.0003724614 0.0003722878 0.0003722390
[1016] 0.0003721079 0.0003720227 0.0003720196 0.0003719239 0.0003713622
[1021] 0.0003711434 0.0003708111 0.0003692306 0.0003688967 0.0003683693
[1026] 0.0003680095 0.0003677973 0.0003673564 0.0003669510 0.0003669394
[1031] 0.0003663508 0.0003660445 0.0003658108 0.0003655841 0.0003655444
[1036] 0.0003652463 0.0003652004 0.0003646945 0.0003646491 0.0003644217
[1041] 0.0003637102 0.0003633671 0.0003632231 0.0003627014 0.0003624094
[1046] 0.0003614685 0.0003612508 0.0003607520 0.0003603882 0.0003593076
[1051] 0.0003589812 0.0003588174 0.0003585554 0.0003584925 0.0003580449
[1056] 0.0003576310 0.0003574987 0.0003569479 0.0003561621 0.0003559521
[1061] 0.0003541606 0.0003538731 0.0003531737 0.0003517316 0.0003510753
[1066] 0.0003508608 0.0003507866 0.0003501264 0.0003500884 0.0003498199
[1071] 0.0003490384 0.0003481646 0.0003473826 0.0003472531 0.0003471723
[1076] 0.0003468944 0.0003464309 0.0003461498 0.0003461128 0.0003451938
[1081] 0.0003446428 0.0003445298 0.0003445064 0.0003435712 0.0003430353
[1086] 0.0003426364 0.0003425607 0.0003407116 0.0003406579 0.0003406355
[1091] 0.0003405351 0.0003404966 0.0003402994 0.0003402460 0.0003397266
[1096] 0.0003387707 0.0003387269 0.0003384103 0.0003380201 0.0003379264
[1101] 0.0003377558 0.0003375529 0.0003370540 0.0003365418 0.0003364921
[1106] 0.0003358313 0.0003354180 0.0003351023 0.0003348030 0.0003345403
[1111] 0.0003345241 0.0003344971 0.0003344856 0.0003336759 0.0003335935
[1116] 0.0003334500 0.0003333039 0.0003327760 0.0003326271 0.0003326033
[1121] 0.0003313277 0.0003311141 0.0003302320 0.0003296092 0.0003295755
[1126] 0.0003290849 0.0003290849 0.0003272731 0.0003258129 0.0003255119
[1131] 0.0003252307 0.0003241771

$size
[1] 1132

$requested_coverage
[1] 0.95

$coverage
[1] 0.9500692</code></pre>
<pre class="r"><code>get_cs(ser.default$pip)</code></pre>
<pre><code>$cs
   [1]  773 1303  291  193  607  288  200 1203  315  772  479 1180  286  102
  [15]  184  363 1151  108  163  829  469  296  396 1195 1218   91 1181   85
  [29]   87  194   78   97   16  723 1053  554  870  707  126  917  358  810
  [43]  668   84  470 1282  896  468 1312  662  347  729  645 1039   52   83
  [57]  445  461 1212  591  129  616  780  738  471  555  357  266  154  218
  [71]  774  915 1045  978   10 1215  127 1050   86 1326  899  691   93 1217
  [85] 1294  537 1020 1026  331  309  788  801  156  622  405  703 1321  795
  [99]   89  186 1182  256  253  572  850 1044  229  135 1052  413  406  408
 [113]  592  310  818  945   55  769 1270 1134  111  809  792  808  105  781
 [127]  782  172  954  502  217   80  834  455  980  117  979  273 1191  400
 [141]  814  852  415  902  728  132 1025  620   63 1311  973  379  458  970
 [155]  826  414  614 1139  650  588  189  491  570  832 1329   60  433  750
 [169] 1179 1210  923  920  815  410  412  710 1327  155 1306  531 1209 1288
 [183]  813  254 1188 1302  474 1141   75  360   90 1223  532 1317  708  306
 [197]  881  443  858  387 1200  374   59 1055  573  575   82 1319  901  718
 [211]  305  274  664  720   77  882  853 1193  235  295  647  606  174  386
 [225] 1235  182  124  618  830  640  380  971 1324 1249  403  293  930 1064
 [239] 1318  529  906  318  701  442 1063 1216  370  976  278  268  697  372
 [253]  816  594 1241  375  191  137 1206 1168  743  456  180  883  103  990
 [267]  261 1185 1229  696  168  873 1287  603  857   62  162  946  511 1172
 [281]  628  908 1251 1051 1293  833   94  125   76   92  784  258 1330 1325
 [295]  354 1038  736  325  567  631 1147    6  735  593  912 1221  104 1247
 [309]  715  601  897 1061 1205  271  879  968  299 1177  419  510  587  202
 [323]  595 1176  542  100  134  868  579  747  227 1101  910  643  674  336
 [337] 1148  825  820  819  821  771  700  109  279   95  887  933  277 1245
 [351]  337 1219  807  734  482 1056  150 1057 1255  494 1283 1295  619  505
 [365]  230 1158  791  859  149  597  257 1301  475  365  141  733 1173  911
 [379]  314  160  665  478 1187  981 1090 1320  418 1213  822  787  495  837
 [393] 1065  732  441  982  811   72  444 1165  483 1058  755  540  630  244
 [407] 1254  435  434 1033  436  383  950  423  955  476 1238  151  919  766
 [421] 1224  381  880  651  921 1184 1166 1310  361  578  634   96  101  267
 [435] 1092 1118  580  900  534 1192  528 1208 1243 1019  918  799  641  158
 [449]  928  136   71  544 1107 1059  437  842 1156 1308  463 1274 1135  854
 [463] 1150  142 1032  480 1183  800 1323  181 1190  675 1227  644  789  290
 [477]  238 1222  234  454  856  877  653  313   68  658  211  276  866  843
 [491]  255  239  686 1244  793  352  378  939  649  487  805 1067  213  225
 [505]  869 1104  835  751  669  260  839  841  927  926  798  176  453 1018
 [519]  302  612  581  294  737  632 1220  663  600  670 1060   57   65  846
 [533]   99 1246  489 1000  690  678  864 1174  878  571  524  242 1078 1250
 [547]  303  576 1113  246 1046  627  590  173  602  530  625   64  865   56
 [561]  245 1163  689 1322  431  548 1144   38  430  420  656 1313  783  749
 [575]  133  287  705  344  345  153 1073 1094  931  617  170   73  609 1062
 [589]  223  969 1068  114  130  558 1277 1278  828  448  428 1201  965  376
 [603]  175 1043 1070  760  964  165  377 1178 1031  523 1133  128  346  343
 [617]  496  699  929  199 1112  446  998  549  746 1269  394  898  951  817
 [631]  717 1010  243 1328  863  460 1240  167 1296  432  770 1152  106  356
 [645]  731 1117  553  188  621  611  539  827  844  350  541  450 1242  960
 [659] 1041  503  547  183 1079   51 1097  941 1074  399 1081  466 1100 1066
 [673]  185  983  210  338  481  533  209 1268  712  711  713 1021 1159 1234
 [687]  319 1248  457 1030  895  764  726  262  944  777  404  275 1093  119
 [701] 1022  411 1087 1047  999  796  604  966  636  328  959  543  714  748
 [715]  283 1297  672 1171 1272  236  838  195  113 1252  316 1006  263 1076
 [729]  637  304  695  221  884  397  488  385  212  913 1233  615  459  122
 [743]  958 1194 1013 1014   61  988  687  197  311  847  462  447  849  166
 [757]   58  667 1003  317  164  241  364 1309 1298 1291  744  673  407  247
 [771]  535  366  922  355  739  140 1072  472  767  885  679  425  989  368
 [785]   20  143 1199 1226  362  340 1012 1002  177 1028  147  159  947 1001
 [799]  741 1265  934 1145  861   79  633  639  493  292  477 1204  389  688
 [813]  339   98  515  613 1259  179  312  599  359  341  903 1275  761  598
 [827] 1138  506  409   81  577  704  949 1091  574  569  232  514   35 1131
 [841]  157 1109  624  702 1089 1161  335 1170  321 1088  501  894  804 1085
 [855]  546 1132   49    2   14 1169  725 1314  417   69   42  762 1290  427
 [869]   67  742  753  874 1128  416 1279  556  557  957  520 1207  138 1084
 [883]  464  214  995 1103  187    4 1121  564  525  642   15  551 1164  953
 [897]  756  775  722  585 1267  330 1110  694 1142 1027  486  148  518 1011
 [911]  388  583  759  840  987  265  709  327  107  566 1196 1080  907  785
 [925]  348 1285  716  608  512 1077 1160  390  519  765 1005  984  526 1162
 [939]  806  752   70  249 1316  421  745 1257 1260  112  763  635 1236 1149
 [953]  527  845  326 1157 1004  565 1075 1186  281 1264 1300 1258  956  889
 [967]    3  259  429 1140  550  500  972  320  369 1048   74  886 1049  996
 [981]  831  504 1124  424 1154 1108  757  623  422  909  610 1225 1237  171
 [995]  282 1023  660 1116 1083  297  439  300  802  693 1189  914  855  139
[1009]  997  862  803  206 1146  812 1202  322  671 1016 1017 1040   43  192
[1023]  943 1276 1007  272  507 1256  351   33  993  231  285  349  629  498
[1037] 1029  289 1086 1106  280  251  754  924  240  986 1015  449  730  391
[1051]  823  758  384  145 1042   54  786  233  220  940  942  938 1214  509
[1065]  991  778 1009  538  120 1155  353  161  110  205 1253  497   28  264
[1079]  904 1082 1119 1284 1299  401  916 1271   66  596

$prob
        M546       M27372       M27410         M522       M29844       M27412 
0.0768762923 0.0395987944 0.0324937961 0.0265219201 0.0258437303 0.0223514875 
      M19248         M556       M27303       M27416         M706         M653 
0.0126734316 0.0119717982 0.0116316983 0.0115872753 0.0114406160 0.0113793038 
      M27483       M26955        M3634         M525       M18647       M27958 
0.0113718755 0.0094899252 0.0064847680 0.0046615749 0.0046579880 0.0045509324 
      M27917       M26995       M11575         M557       M27126        M1071 
0.0040650816 0.0040364242 0.0036412348 0.0033762924 0.0033749643 0.0033742705 
      M27120       M11980         M684         M700         M703         M820 
0.0033725204 0.0030005175 0.0029676470 0.0028619741 0.0028563971 0.0028258701 
      M13962       M27449        M5907         M913       M41824         M877 
0.0028166554 0.0028032291 0.0026114324 0.0024178521 0.0022078143 0.0021528017 
        M926       M17019         M495       M27945       M27129       M27452 
0.0021488956 0.0021227178 0.0021160449 0.0021149410 0.0019888393 0.0019888367 
      M27128         M702       M10322       M27334         M923         M791 
0.0019881736 0.0017165633 0.0017089771 0.0016982287 0.0016872764 0.0014862094 
        M933       M18795         M994       M26914       M27118       M27150 
0.0014842614 0.0014591001 0.0014314404 0.0014255899 0.0014248738 0.0014241137 
        M677         M643       M13115       M15243        M1063         M971 
0.0014237898 0.0014218939 0.0014112945 0.0014058142 0.0013950818 0.0013652014 
        M920       M27564       M27350         M915       M27076       M27184 
0.0013081476 0.0012941823 0.0012941083 0.0012941001 0.0012940833 0.0012940396 
      M10620       M27130         M565       M27843        M4072       M27348 
0.0012940147 0.0012937267 0.0012936200 0.0012934935 0.0012920414 0.0012920002 
       M7578       M27491        M5946       M11362        M2341       M41835 
0.0011956488 0.0011703322 0.0011634665 0.0011592344 0.0011487384 0.0011411386 
        M626        M5372       M27955       M27709         M811       M18308 
0.0011375845 0.0011274106 0.0011272180 0.0011265929 0.0010465548 0.0010447008 
      M10064        M1072       M27330       M27783       M27429       M27790 
0.0010371478 0.0010344692 0.0010343080 0.0010342233 0.0010342115 0.0010341958 
      M10960         M756       M27335       M27331       M26922       M27362 
0.0010341799 0.0010341340 0.0010341325 0.0010341202 0.0010340172 0.0010339927 
       M1056         M911       M19522       M27581         M656       M27363 
0.0010339494 0.0010339291 0.0010323415 0.0010319837 0.0009485033 0.0009473969 
      M27681       M27898        M1073       M16498       M27084       M27103 
0.0009443605 0.0009438649 0.0009253930 0.0009225349 0.0009168592 0.0009114729 
      M41823        M1087        M1092       M27065       M10272       M27837 
0.0009104397 0.0008886995 0.0008886672 0.0008886672 0.0008880630 0.0008880534 
      M27459       M26952         M524        M6786       M41725       M27342 
0.0008880347 0.0008880322 0.0008880145 0.0008880062 0.0008880049 0.0008879738 
        M504       M27456       M27073       M27455        M5868         M888 
0.0008879666 0.0008879612 0.0008879363 0.0008879271 0.0008879109 0.0008878743 
      M27450       M27332       M26977       M27849       M27838       M27180 
0.0008878743 0.0008878631 0.0008878380 0.0008876800 0.0008876799 0.0008863490 
      M27847         M976       M27553       M27627       M27490       M16289 
0.0008632426 0.0008327388 0.0008308080 0.0008266424 0.0008236087 0.0008235744 
        M813       M27205       M27508       M27209       M27540       M29526 
0.0008217503 0.0008012160 0.0007921013 0.0007917280 0.0007916981 0.0007916471 
      M27018       M27836       M27768       M27680       M27488       M27677 
0.0007915399 0.0007914963 0.0007914602 0.0007914564 0.0007914405 0.0007913828 
        M671       M29536       M12484       M29612         M568       M27061 
0.0007913486 0.0007913377 0.0007913296 0.0007910376 0.0007901793 0.0007863058 
      M27846        M1429       M27642        M1075       M13618       M27352 
0.0007505571 0.0007409103 0.0007389385 0.0007375197 0.0007368260 0.0007353865 
      M27893       M27841       M27361        M1825       M27317       M27882 
0.0007337727 0.0007335520 0.0007333316 0.0007332539 0.0007332073 0.0007330718 
      M27137       M27174       M27950       M14966       M27509       M27062 
0.0007330453 0.0007330432 0.0007327546 0.0007324546 0.0007222362 0.0007221749 
      M27066       M27764         M603       M27298         M838       M27612 
0.0007221749 0.0007218514 0.0007217402 0.0007217287 0.0007217168 0.0007216932 
        M663         M930       M27835        M1078       M27051        M1084 
0.0007216841 0.0007216709 0.0007216633 0.0007216483 0.0007216414 0.0007216099 
      M17787       M27336       M27146       M26945         M750       M17967 
0.0007216053 0.0007215754 0.0007215625 0.0007214719 0.0007205099 0.0007116765 
       M8240       M27158       M12347       M27244       M29838       M27853 
0.0007050650 0.0006966091 0.0006907931 0.0006900433 0.0006894374 0.0006696136 
      M27409       M29556       M29584       M27729          M93       M27043 
0.0006695066 0.0006686809 0.0006686023 0.0006685687 0.0006684253 0.0006684253 
      M27897       M27860       M27765       M27088       M27290       M27944 
0.0006684176 0.0006684176 0.0006684118 0.0006684117 0.0006683420 0.0006683408 
      M27105        M6591       M27543       M27273       M27500       M10654 
0.0006683369 0.0006683347 0.0006683255 0.0006683226 0.0006683162 0.0006683116 
      M27208       M27318       M17879       M38996       M27104       M27706 
0.0006683103 0.0006683053 0.0006682827 0.0006682661 0.0006682573 0.0006682518 
      M26910       M14663       M27630         M866        M1018       M27949 
0.0006682212 0.0006682040 0.0006681295 0.0006680517 0.0006680389 0.0006678311 
        M759       M27776       M27905       M10122       M27415         M593 
0.0006677864 0.0006673379 0.0006663432 0.0006662277 0.0006662128 0.0006661913 
      M27108       M27242        M1921       M14236         M538         M660 
0.0006660098 0.0006657689 0.0006657297 0.0006657019 0.0006655973 0.0006655622 
       M4862       M27275         M979       M10775        M9379         M861 
0.0006654339 0.0006566758 0.0006535377 0.0006444269 0.0006441715 0.0006437348 
      M27310       M27489         M751        M5650        M5485       M27875 
0.0006429150 0.0006398957 0.0006391064 0.0006386997 0.0006380103 0.0006366092 
        M649       M27925       M27635       M27608         M780       M27658 
0.0006286479 0.0006268262 0.0006261171 0.0006260226 0.0006259955 0.0006259740 
      M27766       M27736       M13015         M927       M27098       M27689 
0.0006259688 0.0006259058 0.0006258976 0.0006258919 0.0006258889 0.0006258613 
      M27114       M41734         M651         M655       M27555       M27160 
0.0006258552 0.0006258504 0.0006258338 0.0006258301 0.0006258263 0.0006258244 
      M27738       M27782       M27728         M691       M27082       M27911 
0.0006258198 0.0006258088 0.0006258078 0.0006257991 0.0006257813 0.0006257683 
      M27086       M27571       M27060       M17842       M26896       M27326 
0.0006257439 0.0006257361 0.0006257220 0.0006257181 0.0006255777 0.0006255414 
       M5488       M42524         M619       M27032         M841        M9990 
0.0006253592 0.0006252076 0.0006241437 0.0006241247 0.0006163276 0.0006155392 
       M6322       M29837       M10959        M8245         M832       M27413 
0.0006153700 0.0006153043 0.0006144143 0.0006141594 0.0006136276 0.0006135971 
      M12123       M27221       M27094       M27712       M27858       M39011 
0.0006133653 0.0006133446 0.0006131379 0.0006130820 0.0006130730 0.0006129072 
      M42523        M5916       M29793         M935       M41834       M27885 
0.0006127580 0.0006081858 0.0006078395 0.0005919821 0.0005919780 0.0005919525 
      M29827         M967       M41735       M27577       M17670         M569 
0.0005919198 0.0005919164 0.0005918608 0.0005918334 0.0005914447 0.0005913510 
        M526       M27193        M2071       M27541       M26981       M27052 
0.0005910874 0.0005910590 0.0005910225 0.0005910217 0.0005910020 0.0005909906 
      M41721       M27059       M14766       M27058       M27464        M1093 
0.0005909887 0.0005909873 0.0005909839 0.0005909815 0.0005909696 0.0005909684 
       M6121       M27095       M27839         M552        M1014       M27159 
0.0005909276 0.0005909256 0.0005908919 0.0005908864 0.0005908593 0.0005908568 
      M27466       M27195        M2661       M27777       M27719       M12627 
0.0005908407 0.0005907700 0.0005907494 0.0005906286 0.0005906160 0.0005905402 
      M41731         M570       M27514       M27512       M27521       M27417 
0.0005904406 0.0005900820 0.0005899664 0.0005899515 0.0005899168 0.0005874831 
        M966       M27161         M889       M10679       M27445       M27939 
0.0005791362 0.0005762426 0.0005741584 0.0005736120 0.0005707836 0.0005707670 
      M27171        M1000         M549       M27884       M29846         M736 
0.0005637671 0.0005627188 0.0005626790 0.0005626662 0.0005626584 0.0005626554 
      M27236       M27511        M4974       M27518       M26940         M562 
0.0005625738 0.0005623595 0.0005623531 0.0005623268 0.0005621997 0.0005621896 
      M27067       M27856       M29766       M27569       M27178       M27404 
0.0005620349 0.0005620345 0.0005620206 0.0005620068 0.0005619919 0.0005619888 
      M14582       M27235       M27700       M27240       M27818       M27241 
0.0005619799 0.0005619797 0.0005619763 0.0005619288 0.0005618523 0.0005618501 
      M27851       M27111         M801         M957       M27074       M27249 
0.0005618396 0.0005618386 0.0005618348 0.0005618286 0.0005617876 0.0005617578 
      M27274       M27889       M26967       M27028       M27149       M27494 
0.0005617506 0.0005617364 0.0005617279 0.0005617105 0.0005616845 0.0005614792 
      M27745       M27761       M27299       M27183       M27525       M12101 
0.0005613166 0.0005612977 0.0005610179 0.0005609380 0.0005608702 0.0005608575 
       M1262        M9450         M925       M27620        M4904       M27487 
0.0005606806 0.0005596966 0.0005582866 0.0005540217 0.0005500581 0.0005461620 
        M738       M18193       M14301       M29823       M27247       M27524 
0.0005392939 0.0005378555 0.0005377884 0.0005376260 0.0005376203 0.0005374408 
       M7257        M1015       M27176       M29814       M26939       M27517 
0.0005374361 0.0005373270 0.0005372457 0.0005372416 0.0005372346 0.0005371873 
      M27513       M27824       M27519       M27177       M27638       M27189 
0.0005371743 0.0005371588 0.0005371439 0.0005370864 0.0005370787 0.0005370592 
      M27402         M630       M27631       M27481       M26924       M27830 
0.0005370361 0.0005370178 0.0005370128 0.0005370054 0.0005369792 0.0005369418 
      M27561       M27908         M814       M27085       M27121         M645 
0.0005369399 0.0005369283 0.0005369190 0.0005369096 0.0005369094 0.0005368982 
      M27566       M27420       M11184       M27604       M27901         M975 
0.0005368959 0.0005368810 0.0005368178 0.0005368063 0.0005367876 0.0005366286 
      M42522       M26907       M27535       M29851         M542       M27789 
0.0005365929 0.0005365453 0.0005365291 0.0005360205 0.0005360002 0.0005359877 
        M928         M932         M666         M627       M27629       M27110 
0.0005358653 0.0005358653 0.0005357880 0.0005356903 0.0005356768 0.0005352895 
      M27401       M11773       M27740       M27152       M27229       M27033 
0.0005351578 0.0005349373 0.0005331033 0.0005276733 0.0005160867 0.0005160044 
        M970       M27774       M27202         M719       M27522       M26971 
0.0005158657 0.0005158320 0.0005157767 0.0005156959 0.0005156223 0.0005156176 
      M27403        M3656         M657       M39008         M732       M27132 
0.0005155577 0.0005155273 0.0005155227 0.0005155147 0.0005155145 0.0005154909 
      M27045         M794       M27808       M19193         M639       M27682 
0.0005154714 0.0005153840 0.0005153733 0.0005153723 0.0005153717 0.0005153666 
      M27239       M26947        M1016       M26941       M27435       M27461 
0.0005153655 0.0005153617 0.0005153538 0.0005153414 0.0005153096 0.0005153064 
       M1441       M27263       M27182       M11932         M631         M954 
0.0005152303 0.0005151131 0.0005149579 0.0005142381 0.0005133349 0.0005086747 
        M614       M27734       M27102       M27468       M26902       M27081 
0.0005086620 0.0005072210 0.0005067292 0.0005064949 0.0005064394 0.0005064009 
      M27708       M27151         M744         M787         M837       M27166 
0.0005063619 0.0005039744 0.0004970788 0.0004970633 0.0004967736 0.0004967197 
      M27661         M953       M18175        M4280        M8410         M716 
0.0004966687 0.0004966613 0.0004966303 0.0004966151 0.0004965972 0.0004965537 
      M27855       M27293        M2286       M26965         M658       M27930 
0.0004965501 0.0004965465 0.0004965370 0.0004965288 0.0004965177 0.0004965056 
        M622         M511       M27842       M27881       M27424       M27164 
0.0004965035 0.0004964370 0.0004962959 0.0004962928 0.0004962781 0.0004962134 
        M924         M919         M942         M982       M18810       M27454 
0.0004962094 0.0004960622 0.0004960011 0.0004959741 0.0004958701 0.0004949693 
      M10450       M27203         M587       M27216         M921       M27243 
0.0004946604 0.0004940697 0.0004915162 0.0004852875 0.0004824023 0.0004815309 
      M27089       M42520       M27020       M27580         M859       M27442 
0.0004815152 0.0004813904 0.0004805946 0.0004805277 0.0004804513 0.0004802560 
      M26929       M38998       M27948       M19135        M3862         M936 
0.0004801072 0.0004800380 0.0004799835 0.0004799765 0.0004799730 0.0004799612 
      M26983         M583       M11063       M15096       M27819        M1926 
0.0004799506 0.0004799434 0.0004798325 0.0004798071 0.0004798005 0.0004797825 
        M842       M27750         M636       M29701       M29847        M7223 
0.0004797684 0.0004796681 0.0004795640 0.0004795344 0.0004795214 0.0004794631 
      M27360       M27873       M27918       M27656        M2243       M15195 
0.0004792940 0.0004792740 0.0004791256 0.0004788126 0.0004767543 0.0004764624 
      M27460         M907         M670        M1003       M27567       M17243 
0.0004760574 0.0004749604 0.0004694287 0.0004656662 0.0004656200 0.0004654973 
      M27029       M39015        M1044       M27237       M19582       M27479 
0.0004654653 0.0004652559 0.0004652278 0.0004651522 0.0004649606 0.0004649093 
      M27940       M27198         M514        M5957       M27946        M2844 
0.0004648753 0.0004648721 0.0004648589 0.0004648521 0.0004648437 0.0004648401 
       M1088       M27423       M27385       M27880       M27934       M27258 
0.0004648358 0.0004647827 0.0004647422 0.0004647227 0.0004646945 0.0004646831 
       M7169       M27515       M27520       M27083       M27407       M41733 
0.0004646245 0.0004645860 0.0004645513 0.0004645164 0.0004645164 0.0004642702 
        M537       M27780         M749       M26976         M959       M27319 
0.0004633538 0.0004608332 0.0004605758 0.0004600454 0.0004598136 0.0004583283 
        M797       M27746       M42517        M1062       M27140       M27408 
0.0004574292 0.0004560886 0.0004555443 0.0004550325 0.0004545052 0.0004542876 
      M26899       M27537       M27462         M532        M9648       M27023 
0.0004524996 0.0004524849 0.0004520364 0.0004518691 0.0004517090 0.0004517084 
      M29813       M27343       M27828       M27170        M1662       M42525 
0.0004517075 0.0004516764 0.0004516413 0.0004516153 0.0004515908 0.0004515615 
      M29832       M17694       M13408       M27053       M27803         M685 
0.0004515522 0.0004514901 0.0004514875 0.0004514293 0.0004512860 0.0004512671 
      M12256       M26909       M27523       M27510         M519         M853 
0.0004512640 0.0004512136 0.0004511970 0.0004511865 0.0004510508 0.0004497129 
      M27285         M840       M27864       M14309       M41817         M506 
0.0004462018 0.0004445082 0.0004420608 0.0004420412 0.0004411877 0.0004409795 
      M27210       M27393       M14990       M38995       M27639        M1050 
0.0004408750 0.0004408699 0.0004400673 0.0004400415 0.0004400095 0.0004397899 
      M27645        M1055         M559       M26978         M497       M14981 
0.0004397282 0.0004396862 0.0004396277 0.0004395317 0.0004394891 0.0004394665 
      M27628         M934         M964       M27941       M27831         M547 
0.0004394533 0.0004394330 0.0004394063 0.0004393780 0.0004393702 0.0004393318 
        M188       M29808        M4281       M29849         M760       M13897 
0.0004392568 0.0004390385 0.0004390218 0.0004386886 0.0004384341 0.0004383100 
       M2881        M9461       M39003       M27419       M29583        M6768 
0.0004357315 0.0004290007 0.0004289020 0.0004288704 0.0004288632 0.0004287791 
        M489        M1040       M27633       M27586       M29848       M27232 
0.0004286561 0.0004286525 0.0004286288 0.0004285980 0.0004285938 0.0004283258 
      M27933       M27434       M29535         M708         M591       M27788 
0.0004282390 0.0004281885 0.0004281751 0.0004281399 0.0004281269 0.0004279405 
        M673       M27206       M27865       M16523       M39002         M878 
0.0004277268 0.0004276011 0.0004275504 0.0004274355 0.0004270147 0.0004251004 
      M27485       M27493         M625       M27756       M27857       M41830 
0.0004247335 0.0004244922 0.0004243561 0.0004189352 0.0004186940 0.0004185266 
      M27937       M27943       M27527       M27526       M27528       M26942 
0.0004184957 0.0004184821 0.0004184548 0.0004184458 0.0004184247 0.0004184129 
      M27323       M27634       M27428         M940       M26993       M27805 
0.0004184030 0.0004183187 0.0004182737 0.0004181961 0.0004181902 0.0004180771 
      M26917         M571       M27616        M5876         M903        M6467 
0.0004180554 0.0004178914 0.0004177626 0.0004173299 0.0004168934 0.0004149453 
      M29842         M512       M41732        M1039       M26923       M27049 
0.0004104321 0.0004099845 0.0004098113 0.0004097788 0.0004097614 0.0004097135 
      M27739       M18415       M41820       M27505         M690       M29824 
0.0004097135 0.0004096437 0.0004096098 0.0004096007 0.0004095337 0.0004093876 
       M1011       M27191       M27583       M29617       M27529       M27879 
0.0004093557 0.0004092788 0.0004092210 0.0004091878 0.0004091782 0.0004091195 
      M13492         M456       M27816        M5967         M991       M19752 
0.0004090300 0.0004089872 0.0004089644 0.0004089390 0.0004088858 0.0004088588 
      M10735         M543         M601         M539       M27546         M496 
0.0004080578 0.0004072860 0.0004071202 0.0004048702 0.0004008468 0.0004008391 
        M808         M563        M1061       M29714        M3492         M784 
0.0004007943 0.0004006971 0.0004005967 0.0004005516 0.0004004601 0.0004003932 
      M41805         M729        M1877       M27952       M16227       M17541 
0.0004003840 0.0004003752 0.0003985072 0.0003963913 0.0003963037 0.0003962947 
      M27495       M29836       M27593        M1079         M541         M852 
0.0003961251 0.0003949916 0.0003935196 0.0003932320 0.0003931254 0.0003929657 
       M1022        M1024         M592       M41818       M27134        M5336 
0.0003927750 0.0003927730 0.0003926425 0.0003923138 0.0003920485 0.0003919380 
      M27400       M26985       M26972       M26911       M29807       M26982 
0.0003912796 0.0003908928 0.0003901075 0.0003900924 0.0003857519 0.0003856995 
       M3630         M712       M27691       M29854       M26943       M27558 
0.0003856849 0.0003855927 0.0003855585 0.0003853955 0.0003852606 0.0003852164 
      M19861       M27431       M27602         M881        M6177       M14690 
0.0003852001 0.0003851545 0.0003851442 0.0003850973 0.0003849732 0.0003849182 
        M661       M27297       M29816       M27311       M29579       M16597 
0.0003848070 0.0003848024 0.0003842798 0.0003815821 0.0003790519 0.0003786512 
       M1059         M796       M41826       M13748       M27829       M17157 
0.0003786385 0.0003784074 0.0003783289 0.0003782794 0.0003782760 0.0003782627 
      M27418       M27101       M41723       M27309        M5919       M27254 
0.0003781653 0.0003779733 0.0003773274 0.0003738080 0.0003736086 0.0003735119 
      M27030       M27438        M1009         M628       M27716       M27234 
0.0003725749 0.0003724809 0.0003724320 0.0003723515 0.0003721640 0.0003719785 
      M27591       M27792       M29668       M10320       M27437       M29666 
0.0003718505 0.0003717421 0.0003717146 0.0003691592 0.0003663237 0.0003663191 
      M27587       M27757       M27900         M567         M783         M721 
0.0003661553 0.0003660961 0.0003660777 0.0003660592 0.0003660582 0.0003659777 
      M41728       M27772         M773       M42527        M1443       M27538 
0.0003649045 0.0003646229 0.0003642785 0.0003638074 0.0003637300 0.0003611689 
      M27016         M745         M669       M29830       M27010       M27942 
0.0003610363 0.0003608978 0.0003608308 0.0003608150 0.0003607761 0.0003606470 
      M27826       M27480       M27257       M26916       M26969         M779 
0.0003604703 0.0003603426 0.0003600889 0.0003598710 0.0003598382 0.0003589552 
      M26898       M39014       M27041         M962         M633       M27568 
0.0003559455 0.0003557703 0.0003557536 0.0003556284 0.0003555369 0.0003555345 
      M27632        M9694       M27609       M11187       M27637       M27533 
0.0003555187 0.0003554677 0.0003553568 0.0003553383 0.0003552210 0.0003550950 
        M914        M1012       M27812         M692        M5903       M27405 
0.0003550917 0.0003549023 0.0003548913 0.0003548819 0.0003548144 0.0003547642 
        M499         M604       M27557         M918       M27532       M27320 
0.0003546745 0.0003546427 0.0003537217 0.0003537040 0.0003534409 0.0003508130 
      M19312       M27762       M39005       M27534       M27230       M27556 
0.0003506520 0.0003504802 0.0003504447 0.0003504251 0.0003502291 0.0003501756 
        M792       M27536       M26964       M13881        M5895        M5950 
0.0003501617 0.0003500508 0.0003500029 0.0003498911 0.0003496285 0.0003476425 
       M5930       M27217         M875       M27422       M41724       M17814 
0.0003472052 0.0003468545 0.0003467069 0.0003466869 0.0003464248 0.0003463736 
       M5938       M27245       M26974       M26919       M11153       M27921 
0.0003461248 0.0003461097 0.0003460826 0.0003459316 0.0003458063 0.0003456326 
      M29790       M27617       M27168       M29803       M26897       M27038 
0.0003454050 0.0003450265 0.0003449518 0.0003447686 0.0003417290 0.0003416257 
       M1029       M27701       M27588         M646        M3871       M27530 
0.0003415909 0.0003414650 0.0003414150 0.0003413105 0.0003409972 0.0003409808 
      M27820         M678       M41814       M27623       M11725        M5944 
0.0003407892 0.0003407693 0.0003403988 0.0003396684 0.0003389269 0.0003387774 
      M27443       M27605       M27252       M27778        M5906       M17034 
0.0003387465 0.0003387452 0.0003387209 0.0003383364 0.0003382178 0.0003381883 
      M27238       M27641       M29574       M27190         M985       M27482 
0.0003374483 0.0003373897 0.0003373685 0.0003371869 0.0003371522 0.0003368654 
      M29748       M27589         M618       M27000         M554       M27797 
0.0003365977 0.0003333337 0.0003328523 0.0003328011 0.0003326573 0.0003324866 
        M576        M2158       M29569         M739       M27686       M27652 
0.0003316706 0.0003295189 0.0003294950 0.0003291527 0.0003290967 0.0003289096 
        M152         M916       M41738       M29724       M27426       M27592 
0.0003281783 0.0003281279 0.0003279636 0.0003257403 0.0003257078 0.0003253947 
      M29825       M27894       M27748         M544         M891       M27579 
0.0003253788 0.0003252186 0.0003252146 0.0003252135 0.0003251453 0.0003250903 
      M27595       M26975       M27560         M977       M41833         M553 
0.0003228314 0.0003226020 0.0003225116 0.0003220583 0.0003220168 0.0003216029 
      M27395       M14804       M29568        M1074        M1095       M27549 
0.0003193441 0.0003192816 0.0003191238 0.0003191221 0.0003190954 0.0003190052 
      M27255       M27295       M27915       M29777       M27172       M27651 
0.0003189885 0.0003188280 0.0003187816 0.0003186504 0.0003185190 0.0003168728 
        M109        M1005         M611       M27145       M27862       M29840 
0.0003164837 0.0003162732 0.0003160598 0.0003156193 0.0003155488 0.0003151343 
       M8276         M529       M27614       M27200       M27251       M27425 
0.0003132826 0.0003129604 0.0003128705 0.0003127715 0.0003126909 0.0003125149 
      M27698       M27397       M27696       M27381       M27039         M662 
0.0003104896 0.0003104519 0.0003101447 0.0003096330 0.0003092584 0.0003091069 
      M27272       M27458       M27606         M665       M26956       M27771 
0.0003079433 0.0003079096 0.0003075891 0.0003074773 0.0003073802 0.0003069589 
       M5908         M510       M27938       M29594         M620       M29806 
0.0003062918 0.0003056161 0.0003052004 0.0003050294 0.0003029045 0.0003025558 
      M27653       M27554       M27308        M7923        M2843         M490 
0.0003018852 0.0003009575 0.0003002259 0.0002998643 0.0002994843 0.0002994644 
      M27187       M41816       M27231       M27570         M508       M27599 
0.0002988512 0.0002986754 0.0002982312 0.0002981231 0.0002980722 0.0002980185 
      M27394       M29616       M27212       M27662         M805       M41804 
0.0002978961 0.0002975951 0.0002975554 0.0002956953 0.0002955054 0.0002953010 
        M872       M27713       M27499       M27321         M839       M27613 
0.0002943508 0.0002938489 0.0002937919 0.0002936148 0.0002930037 0.0002929063 
      M18311        M2049         M635       M27398         M600       M27399 
0.0002928450 0.0002913214 0.0002912591 0.0002903698 0.0002901727 0.0002897598 
      M27211        M5583         M892       M27001         M607       M27396 
0.0002895271 0.0002893008 0.0002891048 0.0002888024 0.0002884548 0.0002881550 
      M41813       M27590       M12289       M41836       M27670       M18788 
0.0002880602 0.0002878478 0.0002876781 0.0002871890 0.0002867138 0.0002864752 
       M4052       M29829       M26973       M41828       M41827        M1081 
0.0002860724 0.0002856061 0.0002852754 0.0002852327 0.0002852132 0.0002849811 
       M5951        M4953       M27809       M41730       M27647       M27046 
0.0002849703 0.0002849440 0.0002847648 0.0002842965 0.0002838307 0.0002837617 
      M27439       M27008       M27763        M5928       M41822       M26999 
0.0002831790 0.0002822802 0.0002820026 0.0002818243 0.0002817762 0.0002817376 
      M27465       M27601       M27572       M29805       M27795       M27563 
0.0002816248 0.0002812699 0.0002809608 0.0002806806 0.0002800385 0.0002799853 
      M27531         M616        M1002       M27113       M11215       M41832 
0.0002790172 0.0002789671 0.0002788799 0.0002786199 0.0002783110 0.0002782791 
      M27163       M41807       M41829       M27672         M752       M27874 
0.0002781187 0.0002780963 0.0002778556 0.0002771648 0.0002769015 0.0002764833 
        M676       M27215       M27888       M27749       M29841       M27473 
0.0002764770 0.0002763324 0.0002762465 0.0002760335 0.0002756853 0.0002755712 
      M27186       M26953         M938       M27539       M27802       M27815 
0.0002753606 0.0002747729 0.0002746265 0.0002744017 0.0002743557 0.0002726925 
      M27743       M27497       M41808        M4138       M27694        M4669 
0.0002718535 0.0002716987 0.0002709121 0.0002706263 0.0002704532 0.0002699720 
       M1030       M27226       M27219         M897       M27478         M641 
0.0002694534 0.0002686604 0.0002684354 0.0002679298 0.0002671304 0.0002670558 
      M27600       M27233        M5956       M27674         M748        M1090 
0.0002667476 0.0002664799 0.0002661792 0.0002661304 0.0002660526 0.0002660456 
      M29742         M823       M27684       M27207       M27876        M1008 
0.0002658998 0.0002658836 0.0002651070 0.0002648564 0.0002637789 0.0002636566 
      M29689       M27282 
0.0002631730 0.0002631216 

$size
[1] 1088

$requested_coverage
[1] 0.95

$coverage
[1] 0.9501349</code></pre>
<p>The default initialization with a fixed prior variance puts a little
weight on the top marginal enrichments, but the ELBO is not better than
the null model, consequently when we optimize the prior variance we move
towards the null model.</p>
<pre class="r"><code>par(mfrow=c(1,1))
sort(fit.default$pip, decreasing = T) %&gt;% head(10)</code></pre>
<pre><code>    M27372       M546     M29844       M522     M26955       M706     M27410 
0.15795288 0.15553171 0.15480183 0.11390707 0.09481334 0.08853001 0.08756025 
      M653     M27412     M19248 
0.08743512 0.06573662 0.06272065 </code></pre>
<pre class="r"><code>plot(log10(fit.default$pip), -log10(ora$pFishersExact))</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code># prior variances shrunk towards 0
fit.default2$V</code></pre>
<pre><code> [1] 0.006038130 0.006042149 0.006042500 0.006042866 0.006043241 0.006043623
 [7] 0.006044010 0.006044401 0.006044797 0.006045197</code></pre>
<pre class="r"><code>par(mfrow=c(1,1))
sort(ser.default$pip, decreasing = T) %&gt;% head(10)</code></pre>
<pre><code>      M546     M27372     M27410       M522     M29844     M27412     M19248 
0.07687629 0.03959879 0.03249380 0.02652192 0.02584373 0.02235149 0.01267343 
      M556     M27303     M27416 
0.01197180 0.01163170 0.01158728 </code></pre>
<pre class="r"><code>plot(log10(ser.default$pip), -log10(ora$pFishersExact))</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code># prior variances shrunk towards 0
ser.default2$V</code></pre>
<pre><code>[1] 0.0182102</code></pre>
</div>
<div id="vb-ser" class="section level3">
<h3>VB-SER</h3>
<p>For the logistic SER we can fit a univariate regression to each gene
set and achieve a fairly tight bound on the BF with the PG/JJ
variational bound. We can use these approximate BFs to get more accurate
PIPs. We call this VB-SER. The VB-SER yields a 95% credible set with 15
gene sets. There is generally strong correlation between p-values from
the marignal analysis and PIPs (which are proportion to the BFs).
Furthermore, the VB-SER has a favorable BF (~16) against the null model,
whereas the logistic SER did not.</p>
<p>What’s going on here? Before we observed that the logistic SER
produces credible sets that under-cover, yet here we are seeing a that
the VB-SER produces a smaller credible set and a better fit to the
data.</p>
<p><span class="math display">\[
BF_{SER} = \sum BF_i \pi_i = \sum \alpha_i \log BF_i -
D_{KL}(Cat(\alpha) || Cat(\pi))
\]</span></p>
<p>If there is more than one signal in the data we actually get a
stronger <span class="math inline">\(BF_{SER}\)</span>. Suppose we have
two strong candidates for the single effect with equal BFs so that all
of the posterior mass is distributed equally among theses two, and
contrast it with the situation where there is exactly one strong
candidate and <span class="math inline">\(\alpha = e_i\)</span> for some
<span class="math inline">\(i\)</span>. The sum of <span
class="math inline">\(BFs\)</span> is the same, but when there are
multiple strong candidates the posterior probabilities <span
class="math inline">\(\alpha\)</span> are closer the the prior. We pay
$p - $ instead of <span class="math inline">\(\log p\)</span>. While the
model is mis-specified, it’s mispsecified in such that there are many
ways for the SER to be “right”.</p>
<p>The logistic SER does not necessarily benefit as much from this
effect– in the extreme our approximation biases us towards solutions
where <span class="math inline">\(\alpha\)</span> is overly concentrated
on one feature, say <span class="math inline">\(i\)</span>. But if <span
class="math inline">\(BF_i &lt; p\)</span> we would favor the null
model.</p>
<pre class="r"><code>tic(&#39;Fitting VB SER&#39;)
vb_ser &lt;- cache_rds(with(bindata, fit_vb_ser(scale(X, scale=F), y, prior_variance = 10)), file=&#39;vb_ser&#39;)
toc()</code></pre>
<pre><code>Fitting VB SER: 9.035 sec elapsed</code></pre>
<pre class="r"><code>vb_cs &lt;- get_cs(vb_ser$PIP)
vb_cs$cs</code></pre>
<pre><code> [1]  197  195  193  773  479  200   16  623  772 1303  315  291  288  657  607</code></pre>
<pre class="r"><code>intersection &lt;- intersect(marginal_idx, vb_cs$cs)
paste0(length(intersection), &#39;/&#39;, length(marginal_idx), &#39; CS gene sets are significant at FDR &lt; 0.05&#39;)</code></pre>
<pre><code>[1] &quot;14/16 CS gene sets are significant at FDR &lt; 0.05&quot;</code></pre>
<pre class="r"><code>sort(vb_ser$PIP, decreasing = T) %&gt;% head(10)</code></pre>
<pre><code> [1] 0.276559836 0.247047341 0.213646430 0.096026742 0.037983797 0.023878929
 [7] 0.012486109 0.010656032 0.009577287 0.006458620</code></pre>
<pre class="r"><code>par(mfrow=c(1,1))
plot(-log10(ora$pFishersExact), log10(vb_ser$PIP), xlab=&#39;-log10(p)&#39;, ylab=&#39;log10(PIP)&#39;)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/vb-ser-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code># BF
p &lt;- length(vb_ser$BF)
pi &lt;- rep(1/p, p)
exp(matrixStats::logSumExp(vb_ser$BF + log(pi)))</code></pre>
<pre><code>[1] 16.29697</code></pre>
<pre class="r"><code># equivalently
exp(sum(vb_ser$BF * vb_ser$PIP) - categorical_kl(vb_ser$PIP, pi))</code></pre>
<pre><code>[1] 16.29697</code></pre>
</div>
<div id="fixed-qomega-initializtion" class="section level3">
<h3>Fixed <span class="math inline">\(q(\omega)\)</span>
initializtion</h3>
<p>The setting of the PG variational parameters is critical to the
model. The variational approximation is tightest when <span
class="math inline">\(\xi_i = \mathbb E[(x_i^Tb)^2]\)</span>. We explore
the impact of the variational parameters <span
class="math inline">\(\xi\)</span> by fixing them to their optimal value
<em>for a particular gene set</em>.</p>
<p>We say that the uni-variate VB SER detects lots of strong signal.
What if we initialize <span class="math inline">\(q(\omega)\)</span>
using one of these? Let’s find out.</p>
<pre class="r"><code># indices with large positive log BFs
bf_order &lt;- order(desc(vb_ser$BF))

# all the gene sets in the VB SER credible set have strong evidence of association
tibble(geneSet = gs_names[vb_cs$cs], idx=vb_cs$cs, BF = exp(vb_ser$BF[vb_cs$cs])) %&gt;% left_join(ora)</code></pre>
<pre><code>Joining, by = &quot;geneSet&quot;</code></pre>
<pre><code># A tibble: 15 × 12
   geneSet   idx     BF geneLis…¹ geneS…² overlap nGenes propI…³ propI…⁴ oddsR…⁵
   &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
 1 M5336     197 5994.        181     397       0   5927 0       0        0     
 2 M543      195 5355.        181     488       1   5927 0.00552 0.00205  0.0600
 3 M522      193 4631.        181      75      12   5927 0.0663  0.16     6.41  
 4 M546      773 2081.        181      35       8   5927 0.0442  0.229    9.79  
 5 M706      479  823.        181      62      10   5927 0.0552  0.161    6.40  
 6 M19248    200  518.        181      53       9   5927 0.0497  0.170    6.78  
 7 M5907      16  271.        181     145      15   5927 0.0829  0.103    3.90  
 8 M27662    623  231.        181     287       0   5927 0       0        0     
 9 M27416    772  208.        181      47       8   5927 0.0442  0.170    6.77  
10 M27372   1303  140.        181      11       4   5927 0.0221  0.364   18.5   
11 M27303    315  104.        181      29       6   5927 0.0331  0.207    8.53  
12 M27410    291   94.1       181      12       4   5927 0.0221  0.333   16.2   
13 M27412    288   65.9       181      13       4   5927 0.0221  0.308   14.4   
14 M16843    657   54.9       181     470       3   5927 0.0166  0.00638  0.191 
15 M29844    607   50.1       181       7       3   5927 0.0166  0.429   24.2   
# … with 2 more variables: pHypergeometric &lt;dbl&gt;, pFishersExact &lt;dbl&gt;, and
#   abbreviated variable names ¹​geneListSize, ²​geneSetSize, ³​propInList,
#   ⁴​propInSet, ⁵​oddsRatio</code></pre>
<pre class="r"><code># use ELBO - null_elbo is an approximate BF. Not much variation to model in the null elbo so it is tight.
null_elbo &lt;- with(bindata, tail(fit_univariate_vb(X[, 1], y, tau0=1e10)$elbos, 1))
null_ser &lt;- with(bindata, binsusie(X, y, L=1, prior_variance = 1e-10))
null_ser_elbo &lt;- get_elbo(null_ser)</code></pre>
</div>
<div id="fixing-xi" class="section level3">
<h3>Fixing <span class="math inline">\(\xi\)</span></h3>
<p>First let’s do something where we know what will happen. Above we
have fit the univariate VB logistic regression for each gene set. Each
of gene set <span class="math inline">\(i\)</span> we have set of
variational parameters <span class="math inline">\(\xi^{(i)}\)</span>
which parameterize <span class="math inline">\(q(\omega; \xi)\)</span>,
the approximate posterior of the latent polya-gamma variables. As we’ve
discussed, for a fixed <span class="math inline">\(\xi^{(i)}\)</span>
the bound will be tightest for gene set/feature <span
class="math inline">\(i\)</span> and the bound will become progressively
worse for another gene set. If we try and fit the single effect
regression fixing <span class="math inline">\(q(\omega) = q(\omega;
\xi^{(i)})\)</span>, we should expect, at least when gene set <span
class="math inline">\(i\)</span> is predictive that we select that gene
set.</p>
<p>The ELBO for these fits is predictable also:</p>
<p><span class="math display">\[
ELBO_{SER} \leq \sum_j \alpha_j \left(ELBO_j -
\log\frac{\alpha_j}{\pi_j}\right)
\]</span></p>
<p>Assuming our solution puts most of it’s posterior mass on selection
feature <span class="math inline">\(i\)</span> (since we have rigged it
encourage this outcome)</p>
<p><span class="math display">\[
ELBO_{SER} \approx ELBO_i - \log p
\]</span></p>
<p>What’s the point of this? We see that we can “recover” the logistic
SER. With a specific initialization we can fit the ELBO to a local
optima that is better than what we found with the default initialization
(<code>fixed_vs_default &gt; 0</code> gives the difference in ELBOs) for
the top 10 marginal enrichments. Furthermore, the new SER fits give
favorable BFs for the top 4 marginal enrichments</p>
<pre class="r"><code>fixed_xi_ser &lt;- function(idx){
  fit.init &lt;- with(bindata, binsusie_init(X, y, L = 1, center = T, prior_variance = 10))
  fit.init &lt;- set_xi(fit.init, vb_ser$xi[[idx]])
  fit &lt;- fit.binsusie(fit.init, fit_xi = F, fit_prior_variance = F) %&gt;% binsusie_wrapup()
  get_cs(fit$pip)
  get_bf(fit)
  
  res &lt;- list(
    geneSet = gs_names[idx],
    elbo_ser = tail(fit$elbo, 1),
    logSER_BF = get_bf(fit),
    logVB_BF = tail(vb_ser$BF[[idx]]),
    fixed_vs_default = tail(fit$elbo, 1) - null_ser_elbo,
    V = fit$V,
    init_idx = idx
  )
  res &lt;- c(res, get_cs(fit$pip))
  return(res)
}

res &lt;- cache_rds({
  map(bf_order[1:15], fixed_xi_ser) %&gt;%
      tibble() %&gt;%
      unnest_wider(1)
  }, file=&#39;fixed_xi&#39;)
res %&gt;% kbl() %&gt;% kable_styling()</code></pre>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
geneSet
</th>
<th style="text-align:right;">
elbo_ser
</th>
<th style="text-align:right;">
logSER_BF
</th>
<th style="text-align:right;">
logVB_BF
</th>
<th style="text-align:right;">
fixed_vs_default
</th>
<th style="text-align:right;">
V
</th>
<th style="text-align:right;">
init_idx
</th>
<th style="text-align:left;">
cs
</th>
<th style="text-align:left;">
prob
</th>
<th style="text-align:right;">
size
</th>
<th style="text-align:right;">
requested_coverage
</th>
<th style="text-align:right;">
coverage
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
M5336
</td>
<td style="text-align:right;">
-803.3864
</td>
<td style="text-align:right;">
1.5107061
</td>
<td style="text-align:right;">
8.698585
</td>
<td style="text-align:right;">
6.2908879
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
197
</td>
<td style="text-align:left;">
197
</td>
<td style="text-align:left;">
197
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
1.0000000
</td>
</tr>
<tr>
<td style="text-align:left;">
M543
</td>
<td style="text-align:right;">
-805.1865
</td>
<td style="text-align:right;">
1.3947833
</td>
<td style="text-align:right;">
8.585738
</td>
<td style="text-align:right;">
4.4908021
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
195
</td>
<td style="text-align:left;">
195
</td>
<td style="text-align:left;">
195
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
1.0000000
</td>
</tr>
<tr>
<td style="text-align:left;">
M522
</td>
<td style="text-align:right;">
-806.2744
</td>
<td style="text-align:right;">
1.2495206
</td>
<td style="text-align:right;">
8.440480
</td>
<td style="text-align:right;">
3.4028462
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
193
</td>
<td style="text-align:left;">
193
</td>
<td style="text-align:left;">
193
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9982881
</td>
</tr>
<tr>
<td style="text-align:left;">
M546
</td>
<td style="text-align:right;">
-806.9868
</td>
<td style="text-align:right;">
0.4656648
</td>
<td style="text-align:right;">
7.640785
</td>
<td style="text-align:right;">
2.6904728
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
773
</td>
<td style="text-align:left;">
773
</td>
<td style="text-align:left;">
773
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9826287
</td>
</tr>
<tr>
<td style="text-align:left;">
M706
</td>
<td style="text-align:right;">
-807.9263
</td>
<td style="text-align:right;">
-0.4792560
</td>
<td style="text-align:right;">
6.713318
</td>
<td style="text-align:right;">
1.7510235
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
479
</td>
<td style="text-align:left;">
479
</td>
<td style="text-align:left;">
479
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9999160
</td>
</tr>
<tr>
<td style="text-align:left;">
M19248
</td>
<td style="text-align:right;">
-807.7608
</td>
<td style="text-align:right;">
-0.4788443
</td>
<td style="text-align:right;">
6.249154
</td>
<td style="text-align:right;">
1.9165279
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:left;">
200, 193
</td>
<td style="text-align:left;">
200, 193
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9997823
</td>
</tr>
<tr>
<td style="text-align:left;">
M5907
</td>
<td style="text-align:right;">
-808.8008
</td>
<td style="text-align:right;">
-1.5918176
</td>
<td style="text-align:right;">
5.600775
</td>
<td style="text-align:right;">
0.8764404
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:left;">
16
</td>
<td style="text-align:left;">
16
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9999992
</td>
</tr>
<tr>
<td style="text-align:left;">
M27662
</td>
<td style="text-align:right;">
-808.2160
</td>
<td style="text-align:right;">
-1.7475880
</td>
<td style="text-align:right;">
5.442284
</td>
<td style="text-align:right;">
1.4612745
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
623
</td>
<td style="text-align:left;">
623
</td>
<td style="text-align:left;">
623
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
1.0000000
</td>
</tr>
<tr>
<td style="text-align:left;">
M27416
</td>
<td style="text-align:right;">
-808.8510
</td>
<td style="text-align:right;">
-1.2255018
</td>
<td style="text-align:right;">
5.335552
</td>
<td style="text-align:right;">
0.8263217
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
772
</td>
<td style="text-align:left;">
772, 773
</td>
<td style="text-align:left;">
772, 773
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9972290
</td>
</tr>
<tr>
<td style="text-align:left;">
M27372
</td>
<td style="text-align:right;">
-810.3038
</td>
<td style="text-align:right;">
-2.1036620
</td>
<td style="text-align:right;">
4.941574
</td>
<td style="text-align:right;">
-0.6265337
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
1303
</td>
<td style="text-align:left;">
1303, 193, 200, 773, 607, 291, 288, 1180, 479, 184, 102, 286, 1203, 315,
135, 772, 363, 1151, 108, 163
</td>
<td style="text-align:left;">
1303, 193, 200, 773, 607, 291, 288, 1180, 479, 184, 102, 286, 1203, 315,
135, 772, 363, 1151, 108, 163
</td>
<td style="text-align:right;">
20
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9501860
</td>
</tr>
<tr>
<td style="text-align:left;">
M27303
</td>
<td style="text-align:right;">
-810.4434
</td>
<td style="text-align:right;">
-2.4304376
</td>
<td style="text-align:right;">
4.640866
</td>
<td style="text-align:right;">
-0.7661129
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
315
</td>
<td style="text-align:left;">
315, 772, 773
</td>
<td style="text-align:left;">
315, 772, 773
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9628478
</td>
</tr>
<tr>
<td style="text-align:left;">
M27410
</td>
<td style="text-align:right;">
-809.9836
</td>
<td style="text-align:right;">
-1.7549767
</td>
<td style="text-align:right;">
4.544674
</td>
<td style="text-align:right;">
-0.3063412
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
291
</td>
<td style="text-align:left;">
291, 773, 288, 286, 1203, 315, 772
</td>
<td style="text-align:left;">
291, 773, 288, 286, 1203, 315, 772
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9630266
</td>
</tr>
<tr>
<td style="text-align:left;">
M27412
</td>
<td style="text-align:right;">
-809.7989
</td>
<td style="text-align:right;">
-1.8215009
</td>
<td style="text-align:right;">
4.187583
</td>
<td style="text-align:right;">
-0.1216441
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
288
</td>
<td style="text-align:left;">
773, 288, 291, 315, 286, 1203, 772
</td>
<td style="text-align:left;">
773, 288, 291, 315, 286, 1203, 772
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9646175
</td>
</tr>
<tr>
<td style="text-align:left;">
M16843
</td>
<td style="text-align:right;">
-811.6018
</td>
<td style="text-align:right;">
-3.1874063
</td>
<td style="text-align:right;">
4.005102
</td>
<td style="text-align:right;">
-1.9244906
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
657
</td>
<td style="text-align:left;">
657
</td>
<td style="text-align:left;">
657
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
1.0000000
</td>
</tr>
<tr>
<td style="text-align:left;">
M29844
</td>
<td style="text-align:right;">
-813.1082
</td>
<td style="text-align:right;">
-2.6761589
</td>
<td style="text-align:right;">
3.913984
</td>
<td style="text-align:right;">
-3.4309382
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
607
</td>
<td style="text-align:left;">
607, 773, 1303, 291, 193, 288, 1180, 479, 200, 102, 286, 1203, 315, 772,
184, 363, 1151, 108, 78, 163, 829, 469, 296, 396, 1195, 1218, 1181, 91,
85, 87, 97, 194, 16, 723, 1053, 554, 707, 870, 917, 126, 347, 358, 668,
810, 980, 84, 470, 896, 1282, 1045, 1215, 978, 662, 468, 1312, 52, 729,
645, 1039, 83, 461, 1212, 445, 10, 591, 738, 780, 154, 218, 266, 357,
471, 555, 616, 774, 915, 129, 400, 1044, 691, 86, 899, 1326, 127, 1050,
89, 156, 186, 309, 331, 405, 622, 703, 788, 795, 801, 1020, 1026, 1321,
93, 537, 1217, 1294, 979, 615, 253, 256, 572, 1182, 229, 850, 1052, 55,
80, 105, 111, 172, 217, 310, 406, 408, 413, 502, 592, 769, 781, 782,
792, 808, 809, 818, 945, 954, 1134, 1270, 189, 135, 834, 306, 455, 117,
273, 1191, 63, 132, 414, 458, 620, 814, 826, 902, 379, 415, 728, 852,
970, 973, 1025, 1311, 640, 370, 976, 614, 650, 1139, 60, 433, 491, 570,
588, 750, 832, 920, 923, 1179, 1210, 1329, 710, 813, 1188, 1288, 75, 90,
155, 254, 360, 410, 412, 474, 531, 815, 1141, 1209, 1302, 1306, 1327,
76, 532, 1317, 1223, 708, 881, 59, 77, 82, 124, 174, 182, 235, 274, 295,
305, 374, 380, 386, 387, 403, 573, 575, 606, 618, 647, 664, 718, 720,
853, 882, 901, 1055, 1193, 1200, 1235, 1319, 293, 443, 529, 830, 858,
906, 971, 1064, 1249, 1318, 1324, 277, 981, 930, 279, 732, 982, 278,
442, 1063, 1216, 268, 372, 701, 697, 62, 103, 137, 162, 168, 180, 191,
261, 375, 456, 511, 594, 603, 628, 696, 743, 857, 873, 883, 908, 946,
990, 1051, 1168, 1172, 1185, 1206, 1229, 1241, 1251, 1287, 1293, 833,
94, 125, 258, 325, 354, 567, 631, 736, 784, 1038, 1147, 1325, 1330, 6,
92, 735, 816, 100, 104, 134, 202, 227, 271, 299, 336, 419, 510, 542,
579, 587, 593, 595, 601, 643, 674, 715, 747, 771, 819, 820, 821, 825,
868, 879, 897, 910, 912, 968, 1061, 1101, 1148, 1176, 1177, 1205, 1221,
1247, 276, 700, 887, 933, 95, 109, 141, 149, 160, 230, 257, 314, 337,
365, 418, 475, 478, 482, 494, 495, 505, 597, 619, 665, 733, 734, 787,
791, 807, 822, 837, 859, 911, 1056, 1057, 1065, 1090, 1158, 1173, 1187,
1213, 1219, 1245, 1255, 1283, 1301, 1320, 877, 318, 150, 1295, 627,
1046, 441, 96, 101, 151, 244, 267, 361, 381, 383, 423, 434, 435, 436,
476, 483, 534, 540, 578, 630, 634, 641, 651, 755, 766, 799, 880, 900,
919, 921, 950, 955, 1019, 1033, 1058, 1092, 1118, 1165, 1166, 1184,
1192, 1224, 1238, 1254, 1310, 811, 72, 444, 528, 580, 918, 1208, 1243,
158, 223, 71, 136, 142, 181, 238, 290, 437, 463, 480, 544, 644, 675,
789, 800, 842, 854, 928, 1032, 1059, 1107, 1135, 1150, 1156, 1183, 1190,
1222, 1227, 1274, 1308, 1323, 68, 211, 313, 454, 653, 658, 856, 213,
225, 239, 255, 260, 352, 378, 453, 487, 649, 669, 686, 751, 793, 798,
805, 835, 839, 841, 843, 866, 869, 926, 927, 939, 1018, 1067, 1104,
1244, 176, 234, 294, 581, 632, 737, 57, 65, 99, 242, 303, 489, 524, 571,
576, 600, 602, 663, 670, 678, 690, 846, 864, 878, 1000, 1060, 1078,
1113, 1174, 1220, 1246, 1250, 302, 1112, 173, 246, 590, 612, 38, 56, 64,
133, 153, 245, 287, 344, 345, 420, 430, 431, 530, 548, 625, 656, 689,
705, 749, 783, 865, 931, 1062, 1073, 1094, 1144, 1163, 1313, 1322, 73,
170, 609, 617, 558, 114, 130, 969, 1068, 1066, 128, 165, 175, 199, 343,
346, 376, 377, 428, 448, 496, 523, 699, 760, 828, 964, 965, 1031, 1043,
1070, 1133, 1178, 1201, 1277, 1278, 929, 446, 549, 746, 1269, 106, 167,
188, 243, 356, 394, 432, 460, 553, 717, 731, 770, 817, 863, 898, 951,
1010, 1117, 1152, 1240, 1296, 1328, 621, 1022, 998, 51, 183, 350, 399,
450, 466, 503, 539, 541, 547, 611, 827, 844, 941, 960, 1041, 1074, 1079,
1081, 1097, 1100, 1242, 185, 210, 983, 366, 209, 262, 319, 338, 457,
481, 533, 604, 711, 712, 713, 726, 764, 895, 944, 1021, 1030, 1159,
1234, 1248, 1268, 195, 777, 113, 119, 236, 283, 328, 411, 543, 636, 672,
714, 748, 796, 838, 959, 966, 1087, 1171, 1272, 1297, 275, 404, 1047,
1093, 999, 1252, 221, 263, 304, 316, 397, 637, 695, 884, 1006, 1076,
488, 197, 212, 385, 913, 1233, 61, 122, 459, 687, 958, 988, 1013, 1014,
1194, 847, 447, 462, 58, 164, 166, 241, 247, 317, 364, 407, 667, 673,
744, 849, 1003, 1291, 1298, 1309, 535, 140, 355, 425, 472, 679, 739,
767, 885, 922, 989, 1072, 292, 493, 20, 143, 368, 14, 147, 177, 340,
362, 1002, 1012, 1028, 1199, 1226, 79, 741, 861, 934, 947, 1001, 1145,
1265, 159, 35, 477, 633, 639, 642, 98, 179, 312, 339, 341, 359, 389,
515, 599, 613, 688, 1204, 1259, 725, 1314, 81, 409, 506, 514, 624, 761,
903, 949, 1089, 1091, 1131, 1138, 598, 157, 232, 569, 1109, 1275, 311,
574, 704, 702, 49, 321, 335, 501, 546, 804, 894, 1085, 1088, 1132, 1161,
1170, 577, 187, 69, 417, 1169, 67, 416, 427, 742, 753, 762, 874, 1128,
1290, 42, 2, 214, 520, 957, 1279, 138, 464, 556, 557, 995, 1084, 1103,
1207, 4, 525, 564, 1121, 629, 585, 722, 756, 775, 953, 1164, 1267, 551,
330, 694, 1027, 1110, 1142, 486, 15, 148, 388, 518, 583, 987, 1011, 759,
840, 107, 265, 327, 566, 709, 1080
</td>
<td style="text-align:left;">
607, 773, 1303, 291, 193, 288, 1180, 479, 200, 102, 286, 1203, 315, 772,
184, 363, 1151, 108, 78, 163, 829, 469, 296, 396, 1195, 1218, 1181, 91,
85, 87, 97, 194, 16, 723, 1053, 554, 707, 870, 917, 126, 347, 358, 668,
810, 980, 84, 470, 896, 1282, 1045, 1215, 978, 662, 468, 1312, 52, 729,
645, 1039, 83, 461, 1212, 445, 10, 591, 738, 780, 154, 218, 266, 357,
471, 555, 616, 774, 915, 129, 400, 1044, 691, 86, 899, 1326, 127, 1050,
89, 156, 186, 309, 331, 405, 622, 703, 788, 795, 801, 1020, 1026, 1321,
93, 537, 1217, 1294, 979, 615, 253, 256, 572, 1182, 229, 850, 1052, 55,
80, 105, 111, 172, 217, 310, 406, 408, 413, 502, 592, 769, 781, 782,
792, 808, 809, 818, 945, 954, 1134, 1270, 189, 135, 834, 306, 455, 117,
273, 1191, 63, 132, 414, 458, 620, 814, 826, 902, 379, 415, 728, 852,
970, 973, 1025, 1311, 640, 370, 976, 614, 650, 1139, 60, 433, 491, 570,
588, 750, 832, 920, 923, 1179, 1210, 1329, 710, 813, 1188, 1288, 75, 90,
155, 254, 360, 410, 412, 474, 531, 815, 1141, 1209, 1302, 1306, 1327,
76, 532, 1317, 1223, 708, 881, 59, 77, 82, 124, 174, 182, 235, 274, 295,
305, 374, 380, 386, 387, 403, 573, 575, 606, 618, 647, 664, 718, 720,
853, 882, 901, 1055, 1193, 1200, 1235, 1319, 293, 443, 529, 830, 858,
906, 971, 1064, 1249, 1318, 1324, 277, 981, 930, 279, 732, 982, 278,
442, 1063, 1216, 268, 372, 701, 697, 62, 103, 137, 162, 168, 180, 191,
261, 375, 456, 511, 594, 603, 628, 696, 743, 857, 873, 883, 908, 946,
990, 1051, 1168, 1172, 1185, 1206, 1229, 1241, 1251, 1287, 1293, 833,
94, 125, 258, 325, 354, 567, 631, 736, 784, 1038, 1147, 1325, 1330, 6,
92, 735, 816, 100, 104, 134, 202, 227, 271, 299, 336, 419, 510, 542,
579, 587, 593, 595, 601, 643, 674, 715, 747, 771, 819, 820, 821, 825,
868, 879, 897, 910, 912, 968, 1061, 1101, 1148, 1176, 1177, 1205, 1221,
1247, 276, 700, 887, 933, 95, 109, 141, 149, 160, 230, 257, 314, 337,
365, 418, 475, 478, 482, 494, 495, 505, 597, 619, 665, 733, 734, 787,
791, 807, 822, 837, 859, 911, 1056, 1057, 1065, 1090, 1158, 1173, 1187,
1213, 1219, 1245, 1255, 1283, 1301, 1320, 877, 318, 150, 1295, 627,
1046, 441, 96, 101, 151, 244, 267, 361, 381, 383, 423, 434, 435, 436,
476, 483, 534, 540, 578, 630, 634, 641, 651, 755, 766, 799, 880, 900,
919, 921, 950, 955, 1019, 1033, 1058, 1092, 1118, 1165, 1166, 1184,
1192, 1224, 1238, 1254, 1310, 811, 72, 444, 528, 580, 918, 1208, 1243,
158, 223, 71, 136, 142, 181, 238, 290, 437, 463, 480, 544, 644, 675,
789, 800, 842, 854, 928, 1032, 1059, 1107, 1135, 1150, 1156, 1183, 1190,
1222, 1227, 1274, 1308, 1323, 68, 211, 313, 454, 653, 658, 856, 213,
225, 239, 255, 260, 352, 378, 453, 487, 649, 669, 686, 751, 793, 798,
805, 835, 839, 841, 843, 866, 869, 926, 927, 939, 1018, 1067, 1104,
1244, 176, 234, 294, 581, 632, 737, 57, 65, 99, 242, 303, 489, 524, 571,
576, 600, 602, 663, 670, 678, 690, 846, 864, 878, 1000, 1060, 1078,
1113, 1174, 1220, 1246, 1250, 302, 1112, 173, 246, 590, 612, 38, 56, 64,
133, 153, 245, 287, 344, 345, 420, 430, 431, 530, 548, 625, 656, 689,
705, 749, 783, 865, 931, 1062, 1073, 1094, 1144, 1163, 1313, 1322, 73,
170, 609, 617, 558, 114, 130, 969, 1068, 1066, 128, 165, 175, 199, 343,
346, 376, 377, 428, 448, 496, 523, 699, 760, 828, 964, 965, 1031, 1043,
1070, 1133, 1178, 1201, 1277, 1278, 929, 446, 549, 746, 1269, 106, 167,
188, 243, 356, 394, 432, 460, 553, 717, 731, 770, 817, 863, 898, 951,
1010, 1117, 1152, 1240, 1296, 1328, 621, 1022, 998, 51, 183, 350, 399,
450, 466, 503, 539, 541, 547, 611, 827, 844, 941, 960, 1041, 1074, 1079,
1081, 1097, 1100, 1242, 185, 210, 983, 366, 209, 262, 319, 338, 457,
481, 533, 604, 711, 712, 713, 726, 764, 895, 944, 1021, 1030, 1159,
1234, 1248, 1268, 195, 777, 113, 119, 236, 283, 328, 411, 543, 636, 672,
714, 748, 796, 838, 959, 966, 1087, 1171, 1272, 1297, 275, 404, 1047,
1093, 999, 1252, 221, 263, 304, 316, 397, 637, 695, 884, 1006, 1076,
488, 197, 212, 385, 913, 1233, 61, 122, 459, 687, 958, 988, 1013, 1014,
1194, 847, 447, 462, 58, 164, 166, 241, 247, 317, 364, 407, 667, 673,
744, 849, 1003, 1291, 1298, 1309, 535, 140, 355, 425, 472, 679, 739,
767, 885, 922, 989, 1072, 292, 493, 20, 143, 368, 14, 147, 177, 340,
362, 1002, 1012, 1028, 1199, 1226, 79, 741, 861, 934, 947, 1001, 1145,
1265, 159, 35, 477, 633, 639, 642, 98, 179, 312, 339, 341, 359, 389,
515, 599, 613, 688, 1204, 1259, 725, 1314, 81, 409, 506, 514, 624, 761,
903, 949, 1089, 1091, 1131, 1138, 598, 157, 232, 569, 1109, 1275, 311,
574, 704, 702, 49, 321, 335, 501, 546, 804, 894, 1085, 1088, 1132, 1161,
1170, 577, 187, 69, 417, 1169, 67, 416, 427, 742, 753, 762, 874, 1128,
1290, 42, 2, 214, 520, 957, 1279, 138, 464, 556, 557, 995, 1084, 1103,
1207, 4, 525, 564, 1121, 629, 585, 722, 756, 775, 953, 1164, 1267, 551,
330, 694, 1027, 1110, 1142, 486, 15, 148, 388, 518, 583, 987, 1011, 759,
840, 107, 265, 327, 566, 709, 1080
</td>
<td style="text-align:right;">
922
</td>
<td style="text-align:right;">
0.95
</td>
<td style="text-align:right;">
0.9500484
</td>
</tr>
</tbody>
</table>
</div>
<div id="estimate-xi" class="section level3">
<h3>Estimate <span class="math inline">\(\xi\)</span></h3>
<p>Next we take these fits initialized at each <span
class="math inline">\(\xi^{(i)}\)</span> and allow them to optimize over
the <span class="math inline">\(\xi\)</span> and the prior variance
parameter. For at least the top 9 marginal encroachments, the
“preferred” gene set is included in the credible set.</p>
<pre class="r"><code>init_xi_ser &lt;- function(idx){
  fit.init &lt;- with(bindata, binsusie_init(X, y, L = 1, center = T, prior_variance = 10))
  fit.init &lt;- set_xi(fit.init, vb_ser$xi[[idx]])
  fit &lt;- fit.binsusie(fit.init, fit_xi=F, fit_prior_variance = F)
  fit &lt;- fit.binsusie(fit, fit_xi=T, fit_prior_variance = T) %&gt;% binsusie_wrapup()
  
  res &lt;- list(
    geneSet = gs_names[idx],
    elbo_ser = tail(fit$elbo, 1),
    logSER_BF = get_bf(fit),
    logVB_BF = tail(vb_ser$BF[[idx]]),
    fixed_vs_default = tail(fit$elbo, 1) - null_ser_elbo,
    V = fit$V,
    init_idx = idx
  )
  res &lt;- c(res, get_cs(fit$pip))
  return(res)
}

res2 &lt;- cache_rds({
  map(bf_order[1:15], init_xi_ser) %&gt;%
    tibble() %&gt;%
    unnest_wider(1)
  }, file=&#39;init_xi&#39;)
res2</code></pre>
<pre><code># A tibble: 15 × 12
   geneSet elbo_ser logSER_BF logVB_BF fixed_v…¹     V init_…² cs    prob   size
   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt; &lt;lis&gt; &lt;lis&gt; &lt;dbl&gt;
 1 M5336      -808.    1.52       8.70    1.52   8.25      197 &lt;int&gt; &lt;int&gt;     1
 2 M543       -808.    1.46       8.59    1.47   5.65      195 &lt;int&gt; &lt;int&gt;     1
 3 M522       -808.    1.46       8.44    1.46   3.40      193 &lt;int&gt; &lt;int&gt;     1
 4 M546       -809.    0.560      7.64    0.561  5.06      773 &lt;int&gt; &lt;int&gt;     1
 5 M706       -810.   -0.268      6.71   -0.268  3.39      479 &lt;int&gt; &lt;int&gt;     1
 6 M19248     -810.    0.0323     6.25    0.0329 3.29      200 &lt;int&gt; &lt;int&gt;     2
 7 M5907      -811.   -1.15       5.60   -1.15   1.83       16 &lt;int&gt; &lt;int&gt;     1
 8 M27662     -811.   -1.73       5.44   -1.73   7.95      623 &lt;int&gt; &lt;int&gt;     1
 9 M27416     -810.   -0.451      5.34   -0.450  4.10      772 &lt;int&gt; &lt;int&gt;     2
10 M27372     -812.   -2.02       4.94   -2.02   7.25     1303 &lt;int&gt; &lt;int&gt;    20
11 M27303     -812.   -2.25       4.64   -2.25   4.41      315 &lt;int&gt; &lt;int&gt;     3
12 M27410     -811.   -1.37       4.54   -1.37   5.66      291 &lt;int&gt; &lt;int&gt;     7
13 M27412     -811.   -1.28       4.19   -1.28   5.33      288 &lt;int&gt; &lt;int&gt;     7
14 M16843     -813.   -2.87       4.01   -2.87   2.50      657 &lt;int&gt; &lt;int&gt;     1
15 M29844     -810.   -0.120      3.91   -0.120  0.140     607 &lt;int&gt; &lt;int&gt;  1220
# … with 2 more variables: requested_coverage &lt;dbl&gt;, coverage &lt;dbl&gt;, and
#   abbreviated variable names ¹​fixed_vs_default, ²​init_idx</code></pre>
<pre class="r"><code>res2 %&gt;% head(9) %&gt;% unnest(cs, prob)</code></pre>
<pre><code>Warning: unnest() has a new interface. See ?unnest for details.
Try `df %&gt;% unnest(c(cs, prob))`, with `mutate()` if needed</code></pre>
<pre><code># A tibble: 11 × 12
   geneSet elbo_ser logSER_BF logVB_BF fixed_v…¹     V init_…²    cs  prob  size
   &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
 1 M5336      -808.    1.52       8.70    1.52    8.25     197   197   197     1
 2 M543       -808.    1.46       8.59    1.47    5.65     195   195   195     1
 3 M522       -808.    1.46       8.44    1.46    3.40     193   193   193     1
 4 M546       -809.    0.560      7.64    0.561   5.06     773   773   773     1
 5 M706       -810.   -0.268      6.71   -0.268   3.39     479   479   479     1
 6 M19248     -810.    0.0323     6.25    0.0329  3.29     200   200   200     2
 7 M19248     -810.    0.0323     6.25    0.0329  3.29     200   193   193     2
 8 M5907      -811.   -1.15       5.60   -1.15    1.83      16    16    16     1
 9 M27662     -811.   -1.73       5.44   -1.73    7.95     623   623   623     1
10 M27416     -810.   -0.451      5.34   -0.450   4.10     772   772   772     2
11 M27416     -810.   -0.451      5.34   -0.450   4.10     772   773   773     2
# … with 2 more variables: requested_coverage &lt;dbl&gt;, coverage &lt;dbl&gt;, and
#   abbreviated variable names ¹​fixed_vs_default, ²​init_idx</code></pre>
</div>
<div id="initializing-with-good-predictions" class="section level3">
<h3>Initializing with good predictions</h3>
<div id="fix-xi" class="section level4">
<h4>Fix <span class="math inline">\(\xi\)</span></h4>
<p>Here we take the top few marginal enrichment and fit a a multivariate
regression using <code>glm</code>, we use the predictions + standard
errors to initialize <span class="math inline">\(\xi\)</span>. If we fit
the model holding <span class="math inline">\(\xi\)</span> fixed we see
that logistic SuSiE identifies a few of the strong marginal enrichments,
however the ELBO is worse than the null model.</p>
<p>Next we continue to fit the model, allowing updates for <span
class="math inline">\(\xi\)</span> and estimation of the prior variance
parameters. Now we beat the null model!</p>
<pre class="r"><code>fit_init_xi &lt;- cache_rds({
  top_bf &lt;- order(desc(vb_ser$BF))[1:10]
  heatmap(cor(as.matrix(bindata$X[, top_bf])), scale=&#39;none&#39;)
  
  glm_fit &lt;- with(bindata, glm(y ~ as.matrix(X[, top_bf]), family = &#39;binomial&#39;))
  pred &lt;- predict(glm_fit, se.fit = T)
  
  xi &lt;- sqrt(pred$fit^2 + pred$se.fit^2)
  xi[xi &gt; 10] &lt;- 10  # truncate
  hist(xi)
  
  fit.init &lt;- with(bindata, binsusie_init(X, y, L = 10, center = T, prior_variance = 10))
  fit.init &lt;- set_xi(fit.init, xi)  # set the predictions
  fit &lt;- fit.binsusie(fit.init, fit_xi = F, fit_prior_variance = F, fast_elbo = F) %&gt;% binsusie_wrapup()
  fit &lt;- fit.binsusie(fit, fit_xi = T, fit_prior_variance = T, maxiter = 100, tol = 1e-3) %&gt;% binsusie_wrapup()
}, file=&#39;init_xi_glm&#39;)

fit &lt;- fit_init_xi
.monotone(fit$elbo)</code></pre>
<pre><code>[1] TRUE</code></pre>
<pre class="r"><code>tail(fit$elbo, 1) - tail(fit.null$elbo, 1)</code></pre>
<pre><code>[1] 2.539613</code></pre>
<pre class="r"><code>unlist(fit$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5  L6 
197 193 773 479  16 </code></pre>
<p>An alternative approach would be the fix <span
class="math inline">\(\alpha\)</span></p>
<pre class="r"><code>fit_init_alpha &lt;- cache_rds({
  p &lt;- dim(bindata$X)[2]
  L &lt;- 10
  
  top_bf &lt;- order(desc(vb_ser$BF))[1:L]
  alpha_init &lt;- matrix(rep(0, p*L), nrow= L)
  for (l in 1:L){
    alpha_init[l, top_bf[l]] &lt;- 1
  }
  
  fit.init &lt;- with(bindata, binsusie_init(X, y, L = 10, center = T, prior_variance = 10))
  fit.init$params$alpha &lt;- alpha_init
  fit &lt;- fit.binsusie(fit.init, fit_xi = T, fit_prior_variance = F, fast_elbo = T, fit_alpha = F) %&gt;% binsusie_wrapup()
  fit &lt;- fit.binsusie(fit, fit_prior_variance = T, maxiter = 100, tol = 1e-3) %&gt;% binsusie_wrapup()
  fit
}, file=&#39;init_alpha_glm&#39;)


fit &lt;- fit_init_alpha
.monotone(fit$elbo)</code></pre>
<pre><code>[1] TRUE</code></pre>
<pre class="r"><code>tail(fit$elbo, 1) - tail(fit.null$elbo, 1)</code></pre>
<pre><code>[1] 2.565937</code></pre>
<pre class="r"><code>unlist(fit$sets$cs)</code></pre>
<pre><code> L1  L3  L4  L5  L7 
197 193 773 479  16 </code></pre>
<p>Whether we initialize with <span class="math inline">\(\xi\)</span>
or <span class="math inline">\(\alpha\)</span> we get similar solutions
here. Both improve on the null model, and eliminate redundancy as the
model chooses to include 5 of the top 10 marginally enriched gene sets,
the ones that are not selected are correlated with the ones that
are.</p>
<pre class="r"><code>unlist(fit_init_alpha$sets$cs)</code></pre>
<pre><code> L1  L3  L4  L5  L7 
197 193 773 479  16 </code></pre>
<pre class="r"><code>unlist(fit_init_xi$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5  L6 
197 193 773 479  16 </code></pre>
<pre class="r"><code>tail(fit_init_alpha$elbo, 1) - tail(fit.null$elbo, 1)</code></pre>
<pre><code>[1] 2.565937</code></pre>
<pre class="r"><code>tail(fit_init_xi$elbo, 1) - tail(fit.null$elbo, 1)</code></pre>
<pre><code>[1] 2.539613</code></pre>
<pre class="r"><code>par(mfrow=c(1,3))
plot(fit_init_xi$params$xi, fit_init_alpha$params$xi, main=&#39;xi&#39;)
plot(compute_Xb.binsusie(fit_init_xi), compute_Xb.binsusie(fit_init_alpha), main=&#39;predictions&#39;)
plot(fit_init_xi$params$tau, fit_init_alpha$params$tau, main=&#39;tau&#39;)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>top_bf &lt;- order(desc(vb_ser$BF))[1:10]

heatmap(cor(
  as.matrix(bindata$X[, unlist(fit_init_xi$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/unnamed-chunk-7-2.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="how-to-initialize" class="section level3">
<h3>How to initialize?</h3>
<p>Personally I favor initializing <span
class="math inline">\(\xi\)</span> since it is easier to set up the
initialization. We can initialize <span
class="math inline">\(\xi\)</span> by taking the top marginal enrichment
(e.g. any gene set with marginal evidence in favor of the alternative
hypothesis) and fitting a joint regression model. We don’t need to worry
about correlation among the included variables because we are interested
in the predictions, not the explanations. To see this point: two
perfectly correlated gene sets would have huge uncertainty in the
estimated effects but not the predictions.</p>
<p>If we wanted to initialize <span
class="math inline">\(\alpha\)</span>, we’d be forced to do some other
(perhaps cruder) variable selection, e.g. a forward selection procedure.
This may be more computationally demanding, and reduces logistic SuSiE
to basically “picking K”. I think also that I like the idea that it is
easier to get good predictions rather than good explanations– but that
generally good explanations will be consistent with the good
predictions. Initializing <span class="math inline">\(\xi\)</span> gives
logistic SuSiE the benefit of seeing what “good prediction” should look
like, and so we are likely to optimize to a “good” local optimum of the
ELBO.</p>
<p>A light-weight approach to initializing the model may be to 1. Screen
for suggestive marginal effects 2. Fit a joint model for across this
subset of gene sets/features 3. Initialize <span
class="math inline">\(\xi = \mathbb E[(X\beta)^2] \approx \hat \mu^2 +
\hat s^2\)</span> where <span class="math inline">\(\hat \mu_i =
x_i^T\hat \beta\)</span> and <span class="math inline">\(\hat
s_i\)</span> is the correspond standard error. 4. Pre-train the model
optimizing other parameters while holding <span
class="math inline">\(\xi\)</span> fixed. In a second stage of
optimization, optimize all parameters.</p>
<pre class="r"><code>init_xi_glm &lt;- function(X, y, marginal_scores, n_features=10, max_xi=5){
  top_bf &lt;- order(desc(marginal_scores))[1:n_features]
  glm_fit &lt;- glm(y ~ as.matrix(X[, top_bf]), family = &#39;binomial&#39;)
  pred &lt;- predict(glm_fit, se.fit = T)
  xi &lt;- sqrt(pred$fit^2 + pred$se.fit^2)
  xi &lt;- pmin(xi, max_xi)
  return(list(
    xi=xi, mu=pred$fit, se=pred$se.fit)
  )
}

binsusie_fit_xi_init &lt;- function(X, y, marginal_scores, n_features=50, L=10, center=T, prior_variance=10){
  fit.init &lt;- binsusie_init(X, y, L = L, center = center, prior_variance = center)
  
  # initialize with (clipped) GLM predictions
  xi_init &lt;- init_xi_glm(X, y, marginal_scores, n_features=n_features)
  fit.init &lt;- set_xi(fit.init, xi_init$xi)  # set the predictions
  
  # warmup 
  fit &lt;- fit.binsusie(
    fit.init, fit_xi = F, fit_prior_variance = F, fast_elbo = F)
  
  # final fit
  fit &lt;- fit.binsusie(fit, fit_xi = T, fit_prior_variance = T, maxiter = 100, tol = 1e-3) %&gt;% binsusie_wrapup()
  .monotone(fit$elbo)
  print(tail(fit$elbo, 1) - tail(fit.null$elbo, 1))
  return(fit)
}</code></pre>
<pre class="r"><code>fit5 &lt;- cache_rds(with(bindata, binsusie_fit_xi_init(X, y, marginal_scores=vb_ser$BF, n_features=5)), file=&#39;init_xi_top5&#39;)
fit10 &lt;- cache_rds(with(bindata, binsusie_fit_xi_init(X, y, marginal_scores=vb_ser$BF, n_features=10)), file=&#39;init_xi_top10&#39;)
fit20 &lt;- cache_rds(with(bindata, binsusie_fit_xi_init(X, y, marginal_scores=vb_ser$BF, n_features=20)), file=&#39;init_xi_top20&#39;)
fit50 &lt;- cache_rds(with(bindata, binsusie_fit_xi_init(X, y, marginal_scores=vb_ser$BF, n_features=50)), file=&#39;init_xi_top50&#39;)</code></pre>
<pre class="r"><code>unlist(fit5$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5 
195 193 479 773 </code></pre>
<pre class="r"><code>unlist(fit10$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5  L6 
195  16 773 479 193 </code></pre>
<pre class="r"><code>unlist(fit20$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5  L6  L7  L8 
195  16 479 773 193 657 108 </code></pre>
<pre class="r"><code>unlist(fit50$sets$cs)</code></pre>
<pre><code> L2  L3  L4  L5  L6  L7 
197  16 479 193 860 773 </code></pre>
<p>All the fits are identifying a subset of the effects with positive
BFs. Here we plot the correlation of the selected variables with all
gene sets with positive marginal BF (Ordered by marginal BF
largest-smallest left-right)</p>
<pre class="r"><code>n_pos &lt;- sum(vb_ser$BF &gt; 0)
top_bf &lt;- order(desc(vb_ser$BF)) %&gt;% head(n_pos)

heatmap(cor(
  as.matrix(bindata$X[, unlist(fit5$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;, Colv = NA
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit10$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;, Colv = NA
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit20$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;, Colv = NA
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps-3.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit50$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;, Colv = NA
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps-4.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again, with hierarchical clustering applied to marginally enriched
gene sets</p>
<pre class="r"><code>n_pos &lt;- sum(vb_ser$BF &gt; 0)
top_bf &lt;- order(desc(vb_ser$BF)) %&gt;% head(n_pos)

heatmap(cor(
  as.matrix(bindata$X[, unlist(fit5$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit10$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps2-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit20$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps2-3.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>heatmap(cor(
  as.matrix(bindata$X[, unlist(fit50$sets$cs)]),
  as.matrix(bindata$X[, top_bf])),
  scale = &#39;none&#39;
)</code></pre>
<p><img src="figure/logistic_susie_initialization.Rmd/xi_glm_heatmaps2-4.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>order(desc(vb_ser$BF)) %&gt;% head(20)</code></pre>
<pre><code> [1]  197  195  193  773  479  200   16  623  772 1303  315  291  288  657  607
[16]  108 1180  286 1203  860</code></pre>
</div>
<div id="centered-vs-non-centered" class="section level3">
<h3>Centered vs non-centered</h3>
<p>The default initialization gives different results when
<code>X</code> is centered or not. The centered version essentially
learns the null model. The non-centered version has a lower ELBO, yet
has two redundant non-zero effects– perhaps removing one of these
redundant effects could improve the un-centered model– we are paying
double the cost in model complexity for something that could be
described with a single effect.</p>
<pre class="r"><code>tic()
res &lt;- with(bindata, binsusie(X, y, L=5, center = F))
toc()</code></pre>
<pre><code>3.515 sec elapsed</code></pre>
<pre class="r"><code>tic()
res_centered &lt;- with(bindata, binsusie(X, y, L=5, center = T))
toc()</code></pre>
<pre><code>16.114 sec elapsed</code></pre>
<pre class="r"><code>res_centered$sets$cs</code></pre>
<pre><code>NULL</code></pre>
<pre class="r"><code>get_elbo(res_centered)</code></pre>
<pre><code>[1] -809.7256</code></pre>
<pre class="r"><code>get_all_cs(res) %&gt;% tibble() %&gt;% unnest_wider(1) %&gt;% filter(size &lt; 10) %&gt;% unnest(cs, prob)</code></pre>
<pre><code>Warning: unnest() has a new interface. See ?unnest for details.
Try `df %&gt;% unnest(c(cs, prob))`, with `mutate()` if needed</code></pre>
<pre><code># A tibble: 2 × 5
     cs  prob  size requested_coverage coverage
  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;              &lt;dbl&gt;    &lt;dbl&gt;
1   848     1     1               0.95        1
2   848     1     1               0.95        1</code></pre>
<pre class="r"><code>get_elbo(res)</code></pre>
<pre><code>[1] -831.6142</code></pre>
<pre class="r"><code># centered model is basically null model
get_elbo(res_centered) - get_elbo(fit.null)</code></pre>
<pre><code>[1] -0.04834613</code></pre>
<pre class="r"><code># uncentered model has much worse performance
get_elbo(res) - get_elbo(fit.null)</code></pre>
<pre><code>[1] -21.9369</code></pre>
<pre class="r"><code>get_elbo(fit50) - get_elbo(fit.null)</code></pre>
<pre><code>[1] -2.466906</code></pre>
<pre class="r"><code>get_elbo(res_centered) - get_elbo(fit.null)</code></pre>
<pre><code>[1] -0.04834613</code></pre>
<pre class="r"><code>knitr::knit_exit()</code></pre>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
https://docs.mathjax.org/en/latest/web/configuration.html. This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>




</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
