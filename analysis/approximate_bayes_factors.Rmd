---
title: "Approximating Bayes Factors"
author: "Karl Tayeb"
date: "2022-10-10"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r setup}
cache_rds <- purrr::partial(xfun::cache_rds, dir='cache/aproximate_bayes_factors/')
```

## Overview

Here we compare several approximate Bayes factor computations

1. Wakefields ABF
1. Exact BF computed by quadrature (fixed intercept to $\hat b_{0, MLE}$)
1. Variational BF-- fit a univariate logistic regression with polya-gamma VB (also, fixed intercept $\hat b_{0, MLE}$)
1. Exact BF computed by 2d quadrature (we put a Normal prior on the effect and the intercept)
1. Reparameterized Exact BF computed by 2d quadrature (put a normal prior on the effect and reparameterized "intercept")

TLDR: The 1d quadrature and VB show strong agreement. This makes sense because the the fixed intercept model is the target VB is approximating (and then optimizing over intercept as well).

Neither 1D quadrature BF  of VB BF agree with the Wakefield's ABF. And Wakefield's ABF doesn't really agree with either of the 2d quadrature approaches. Furthermore the 2d quadrature BFs are sensitive to the prior we put on the non-effect parameter. This is not surprising for the intercept model, but discouraging for the reparameterized model (where we expect the terms involving the intercept to approximately cancel out in the BF computation).



###  Univariate VB updates

```{r univariate_vb_updates}
sigmoid <- function(x){
  return(1 / (1 + exp(-x)))
}

logodds <- function(p){
  return(log(p) - log(1-p))
}

update_intercept <- function(x, y, mu, tau, xi, delta, tau0){
  kappa = y - 0.5
  xb <- (x * mu)
  omega <- logisticsusie:::pg_mean(1, xi)
  return(sum(kappa - xb * omega)/sum(omega))
}

update_b <- function(x, y, mu, tau, xi, delta, tau0){
  omega <- logisticsusie:::pg_mean(1, xi)
  kappa = y - 0.5
  tau <- sum(omega * x^2) + tau0
  nu <- sum((kappa - omega*delta) * x)
  return(list(mu = nu/tau, tau=tau))
}

update_xi <- function(x, y, mu, tau, xi, delta, tau0){
  xb2 <- x^2 * (mu^2 + 1/tau) + 2*x*mu*delta + delta^2
  return(sqrt(xb2))
}

compute_elbo <- function(x, y, mu, tau, xi, delta, tau0){
  kappa = y - 0.5
  xb <- (x * mu) + delta
  bound <- log(sigmoid(xi)) + (kappa * xb) - (0.5 * xi)
  kl <- logisticsusie:::normal_kl(mu, 1/tau, 0, 1/tau0)
  return(sum(bound) - kl)
}

# include the term that cancels out when xi is up-to-date
compute_elbo2 <- function(x, y, mu, tau, xi, delta, tau0){
  kappa = y - 0.5
  xb <- (x * mu) + delta
  xb2 <- x^2 * (mu^2 + 1/tau) + 2*x*mu*delta + delta^2

  omega <- logisticsusie:::pg_mean(1, xi)
  bound <- log(sigmoid(xi)) + (kappa * xb) - (0.5 * xi) + 0.5 * omega * (xi^2 - xb2)
  kl <- logisticsusie:::normal_kl(mu, 1/tau, 0, 1/tau0)
  return(sum(bound) - kl)
}

# compute the elbo explicitly
compute_elbo3 <- function(x, y, mu, tau, xi, delta, tau0){
  kappa = y - 0.5
  xb <- (x * mu) + delta
  xb2 <- x^2 * (mu^2 + 1/tau) + 2*x*mu*delta + delta^2
  omega <- logisticsusie:::pg_mean(1, xi)
  
  elbo <- sum(-log(2) + kappa * xb - 0.5 * xb2 * omega) -
    logisticsusie:::normal_kl(mu, 1/tau, 0, 1/tau0) -
    sum(logisticsusie:::pg_kl(1, xi))
  return(elbo)
}

.monotone <- logisticsusie:::.monotone

fit_univariate_vb <- function(x, y, delta.init=0, tau0=1, estimate_intercept=T){
  # init 
  mu <- 0
  tau <- 1
  delta <- delta.init
  xi <- update_xi(x, y, mu, tau, 1, delta, tau0)
  xi <- pmax(xi, 1e-3)
  
  elbos <- compute_elbo(x, y, mu, tau, xi, delta, tau0)
  for(i in seq(20)){
    # rep
    if(estimate_intercept){
      delta <- update_intercept(x, y, mu, tau, xi, delta, tau0)
    }
    
    b <- update_b(x, y, mu, tau, xi, delta, tau0)
    mu <- b$mu
    tau <- b$tau
    
    xi <- update_xi(x, y, mu, tau, xi, delta, tau0)
    elbos <- c(elbos, compute_elbo(x, y, mu, tau, xi, delta, tau0))
    
    if(diff(tail(elbos, 2)) < 1e-5){
      break
    }
  }
  
  converged = diff(tail(elbos, 2)) < 1e-5
  monotone = logisticsusie:::.monotone(elbos)
  p_mle <- mean(y)
  BF = exp(tail(elbos, 1) - sum(dbinom(y, 1, p_mle, log = T)))
  return(list(x=x, y=y,
              mu=mu, tau=tau, xi=xi, delta=delta, tau0=tau0,
              BF=BF,
              elbos=elbos,
              converged=converged,
              monotone=monotone))
}
```


```{r compute_wakefield}
# Compute ln(ABF) where ABF is Wakefields Approximate Bayes Factor
compute_lnabf <- function(z, V, W){
  lnabf <- -(log(sqrt((V+W)/V)) + (- 0.5 * z^2 * W / (V+W)))
  return(lnabf)
}

# Compute likelihood conditional on effect size y ~ Bernoulli(sigmoid(x*b + delta))
conditional_likelihood <- function(x, y, b, delta=0, log=F) {
  p <- sigmoid(x*b + delta)
  ll <- sum(dbinom(y, 1, p, log=T))
  if(!log){
    ll <- exp(ll)  # rescale for numerical stability?
  }
  return(ll)
}
```


Here we quickly recreate figure one from Wakefield's ABF paper to confirm we are computing it right.
```{r recreate_abf_figure}
z <- 4
W <- 0.21^2
f <- function(V){exp(compute_lnabf(z, V, W))}
plot(f, xlim = c(0, 0.025))
```



### A note on how I'm computing the quadrature

There are a few packages in R that just give the quadrature points and their associated weights so that you can compute

$$
\int f(x) dx \approx \sum w_i f(x_i)
$$

We are computing the *evidence* $p(y) = \int \left(\prod p(y_i | b, b_0)\right) p(b, b_0) dbdb_0 = \int f(b, b_0) dbd_0$. Because this involved the product of many probabilities it can get quite small and we can run into underflow issues. An option to deal with this is to re scale the function when computing the integral.

Alternatively, since we are dealing with non-negative probabilities, we can compute the sum on the RHS of the approximate inequality on the log scale, which is much better numerically. That is what we do here. The R package `statmod::gaus.quad.prob` supply the quadrature points and weights so this computation is easy to carry out.

$$
\log \sum w_i f(x_i) = \log\sum\exp\{\log f(x_i) + \log(w_i)\}
$$


## Comparing exact and approximate Bayes factors

### VB vs 1d quadrature

We compute the evidence $p(y)$ numerically via quadrature, and approximately via Wakefields ABF and the Polya-Gamma/Jaakkola-Jordan variational approximation. We confirm that the ELBO is indeed a lower bound for the evidence. Furthermore, for this simple example the bound is quite tight.

```{r}
# simulate some data
sigmoid <- function(x) {1/(1 + exp(-x))}
x <- rnorm(1000)
b0 <- 0
b1 <- 0.1
y <- rbinom(length(x), 1, sigmoid(x))

# compute MLE
glmfit <- glm(y ~ x, family='binomial')
coef <- summary(glmfit)$coef
b0_mle <- coef[1,1]

# evidence compute via quadrature (FIXED INTERCEPT)
evidence <- gseasusie:::compute_evidence_quadrature_fixed_b0(
  as.matrix(x, ncol=1), y, 0, b0=b0_mle, params = list(b_mu=0, b_sigma=1), n=2048)

# ELBO via VB (FIXED INTERCEPT)
vb <- fit_univariate_vb(x, y, delta.init=b0_mle, tau0=1, estimate_intercept = F)
elbo <- tail(vb$elbos, 1)

# check its a lower bound
evidence > elbo

# p(y)/ELBO-- ideally close to, but a bit larger than, 1
# since denom is (hopefully) a tight lower bound to the numerator
exp(evidence - elbo)
```


Here we show three different computations of the evidence under the null model. First we simply compute the log likelihood of MLE for the intercept-only model. Second, we compute the evidence in the limiting case where $\sigma_b \approx 0$-- exactly using quadrature and approximately using the VB method. 

The latter two approaches agree perfectly (this is not suprising, since there is nothing really to approximate). I think it should also agree with the log likelihood of the intercept-only MLE, but there is a slight discrepency. 

```{r null_likelihood}
# null likelihod == mLE of intercept only model
null1 <- conditional_likelihood(x, y, 0, logodds(mean(y)), log = T)

# null likelihood as the model with negligible prior variance on b
null2 <- gseasusie:::compute_evidence_quadrature_fixed_b0(
  as.matrix(x, ncol=1), y, 0, b0=b0_mle, params = list(b_mu=0, b_sigma=1e-10), n=2048)

# null likelihood as the model with negligible prior variance on b, VB
nullvb <- fit_univariate_vb(
  x, y, delta.init=b0_mle, tau0=1e10, estimate_intercept = F)
null3 <- tail(nullvb$elbos, 1)

print('Three computations of the null model likelihood')
print(paste(c('Fixed-intercept MLE:', null1)))
print(paste(c('Fixed-intercept 1d null:', null2)))
print(paste(c('Fixed-intercept VB null:', null2)))
```


The evidence computations agree, so the BFs will also agree...

```{r}
quad1d_bf <- evidence - null2
vb_bf <- elbo - null2
print(quad1d_bf)
print(vb_bf)
```



### 2d quadrature vs Wakefield ABF

Evidently there is a pretty big discrepancy between the Wakefield ABF and the BFs computed by 2d quadrature (we specify normal priors on both $b$ and $b_0$).


Here we compute the BF via 2d quadrature. We compute the evidence for the null model by computing the 2d quadrature with $b \sim N(0, \sigma_^2_b \approx 0)$

```{r}
# 2d integral function
compute_2d_bf <- function(x, y, b_sigma, b0_sigma, n=128){
  evidence_h1 <- gseasusie:::compute_evidence_quadrature_2d(
  as.matrix(x, ncol=1), y, offset=0,
  params = list(b_mu=0, b_sigma=b_sigma, b0_mu=0, b0_sigma=b0_sigma), n=n)
  evidence_h0 <- gseasusie:::compute_evidence_quadrature_2d(
    as.matrix(x, ncol=1), y, offset=0,
    params = list(b_mu=0, b_sigma=1e-10, b0_mu=0, b0_sigma=b0_sigma), n=n)
  quad2d_lnbf <- evidence_h1 - evidence_h0
  return(quad2d_lnbf)
}

compute_wakefield <- function(coef, b_sigma){
  compute_lnabf(coef[2, 3], coef[2,2]^2, b_sigma^2)
}

print(paste('ln Wakefield ABF: ,', compute_wakefield(coef, 1)))
print(paste('ln BF (2d quadrature): ,', compute_2d_bf(x, y, 1.0, 1.0, n=1024)))
```

### The BF is sensitive to the prior over the intercept

First, although there is a big gap between the log ABFs and quadrature log BFs, they seem more or less of by a constant when we vary the prior variance of the effect.

```{r}
b_sigma_grid <- seq(0.2, 2, by=0.2)

wakefield_b_sigma <- purrr::map_dbl(b_sigma_grid, ~compute_wakefield(coef, .x))
bfs_2d_b_sigma<- cache_rds(
  purrr::map_dbl(b_sigma_grid, ~compute_2d_bf(x, y, .x, 1, n=512)),
  file='2d_quad_b_sigma'
)

par(mfrow=c(1,2))

plot(
  b_sigma_grid, wakefield_b_sigma,
  type='l',
  ylim=range(c(wakefield_b_sigma, bfs_2d_b_sigma)),
  ylab = 'logBF',
  xlab = 'sigma_b',
  col='blue',
  main='Wakefield (blue) vs 2d quad BF (red)'
)
lines(b_sigma_grid, bfs_2d_b_sigma, col='red')

plot(wakefield_b_sigma, bfs_2d_b_sigma, xlab='Wakefield', ylab='Quad');
abline(0, 1, lty=3, col='red')
```


In contrast, changing the prior on the intercept does not effect the ABF, but it does impact the 2d quadrature BF.

```{r}
b0_sigma_grid <- seq(0.2, 6, by=0.2)

wakefield <- compute_wakefield(coef, 1.)
bfs_2d_b0_sigma<- cache_rds(
  purrr::map_dbl(b0_sigma_grid, ~compute_2d_bf(x, y, 1, .x, n=512)),
  file='2d_quad_b0_sigma'
)

plot(
  b0_sigma_grid, bfs_2d_b0_sigma,
  type='l',
  ylim=range(c(bfs_2d_b0_sigma, wakefield)),
  ylab = 'logBF',
  xlab = 'sigma_b0',
  col='red',
  main='Wakefield (Blue) vs 2d quad BF (Red)'
)
abline(h=wakefield, col='blue')
```


### Reparameterization

Here we pursue a reparameterization of the logistic regression, that reduces the statistical dependence between the estimate of the effect and the intercept. Following the appendix of Wakefield'd ABF paper. We start by considering the logistic regression model:

$$
\log \frac{p_i}{1 - p_i} = b_0 + bx
$$

The MLE for logistic regression (under some conditions) is consistent and asymptotically normal

$$
\begin{bmatrix}\hat b_0 \\ \hat b \end{bmatrix} \sim 
N\left(\begin{bmatrix} b_0 \\  b \end{bmatrix},
\begin{bmatrix} I_{00} & I_{01} \\ I_{10} & I_{11} \end{bmatrix}^{-1}\right)
$$

But we can re-parameterize the model to (asymptotically) get rid of the covariance between these estimates. Specifically we can replace $b_0$ with the parameter $\beta_0 = b_0 + \frac{I_{00}}{I_{01}} b$ which yields

$$
\begin{bmatrix}\hat \beta_0 \\ \hat b \end{bmatrix} \sim 
N\left(\begin{bmatrix} b_0 \\  b \end{bmatrix},
\begin{bmatrix} I_{00} & 0 \\ 0 & I_{11} \end{bmatrix}^{-1}\right)
$$

By specifying independent priors $p(\beta_0, b) = p(\beta_0)p(\beta)$ the posterior factorizes. Wakefield takes advantage of this by only "integrating" over the effect estimate (since the integral involving the intercept cancels in the numerator and denominator). For them, it has the effect of only needing the effect estimate and it's standard error (disregarding the intercept).


$$
BF = \frac{p(\hat\beta, \hat\theta | H_1)}{p(\hat\beta, \hat\theta | H_0)} 
\approx \frac{\int p(\hat \beta | \beta)p(\beta) \times \int p(\hat \theta | \theta)p(\theta)}{\int p(\hat \beta | \beta)p(\beta) \times p(\hat \theta | \theta =0)}
= \frac{\int p(\hat \theta | \theta)p(\theta)}{p(\hat \theta | \theta =0)} = ABF
$$

Below we impliment 2d quadrature for this re-parameterization. We don't get around integrating over two parameters-- but we still hope the $\int p(\hat \beta | \beta)p(\beta)$ will cancel out the numerator and denominator.

```{r}
compute_reparam <- function(x, y){
  glmfit <- glm(y ~ x)
  logits <- predict(glmfit)
  p1p <- sigmoid(logits) * sigmoid(-logits)
  I00 <- - sum(p1p)
  I01 <- -sum(p1p * x)
  return(I01/I00 * coef(glmfit)[2])
}

log_joint_reparam <- function(x, y, b, b0, b_mu, b_sigma, beta_mu, beta_sigma, reparam){
  beta <- b0 + reparam
  log_joint <- conditional_likelihood(x, y, b, b0, log=T) + 
    dnorm(b, b_mu, b_sigma, log=T) + 
    dnorm(beta, beta_mu, beta_sigma, log=T)
  return(log_joint)
}

quad2d_reparam <- function(x, y, b_mu, b_sigma, beta_mu, beta_sigma, n=128){
  q1 <- statmod::gauss.quad.prob(n=n, l=b_mu-10*b_sigma, u = b_mu+10*b_sigma)
  q2 <- statmod::gauss.quad.prob(n=n, l=beta_mu-10*beta_sigma, u = beta_mu+10*beta_sigma)

  reparam <- compute_reparam(x, y)
  # inner integral over b
  quad_b <- function(b0){
    # for fixed b0, evaluate log_joint at quadrature points
    integrand1 <- purrr::map_dbl(q1$nodes, ~log_joint_reparam(
      x, y, .x, b0, b_mu, b_sigma, beta_mu, beta_sigma, reparam))
    res <- matrixStats::logSumExp(integrand1 + log(q1$weights))
    return(res)
  }
  
  # integrate over b0
  integrand2 <- purrr::map_dbl(q2$nodes, quad_b)
  res <- matrixStats::logSumExp(integrand2 + log(q2$weights))
}
```


```{r}
compute_2d_bf_reparam <- function(x, y, b_sigma, b0_sigma, n=128){
  tictoc::tic()
  evidence_reparam <- quad2d_reparam(x, y, 0, b_sigma, 0, b0_sigma, n=n)
  null_reparam <- quad2d_reparam(x, y, 0, 1e-10, 0, b0_sigma, n=n)
  tictoc::toc()
  return(evidence_reparam - null_reparam)
}

b_sigma_grid <- seq(0.2, 2, by=0.2)
wakefield_b_sigma <- purrr::map_dbl(b_sigma_grid, ~compute_wakefield(coef, .x))
bfs_2d_b_sigma<- cache_rds(
  purrr::map_dbl(b_sigma_grid, ~compute_2d_bf_reparam(x, y, .x, 1, n=256)),
  file='2d_quad_b_sigma_reparam'
)


par(mfrow=c(1,2))
plot(
  b_sigma_grid, wakefield_b_sigma,
  type='l',
  ylim=range(c(wakefield_b_sigma, bfs_2d_b_sigma)),
  ylab = 'logBF',
  xlab = 'sigma_b',
  col='blue')
lines(b_sigma_grid, bfs_2d_b_sigma, col='red')
plot(wakefield_b_sigma, bfs_2d_b_sigma, xlab='Wakefield', ylab='2d Quad Reparam');
abline(0, 1, lty=3, col='red')
```


Unfortunately, this does not seem to resolve the discrepency with Wakefields. What is going on? The quadrature BF computation is still dependent on the prior over the intercept.
We don't have access to $I_{01}/I_{00}$ so we approximated it with $I_{01}(\hat b, \hat b_0)/I_{00}(\hat b, \hat b_0)$, and the factorization that ABF computation relies on is only true asymptotically.

Also, unlike the intercept parameterization, now the gap between log BFs seems to vary as a function of the prior effect variance.

```{r}
b0_sigma_grid <- seq(0.2, 6, by=0.2)

wakefield <- compute_wakefield(coef, 1.)
bfs_2d_b0_sigma<- cache_rds(
  purrr::map_dbl(b0_sigma_grid, ~compute_2d_bf_reparam(x, y, 1, .x, n=128)),
  file='2d_quad_b0_sigma_reparam'
)

par(mfrow=c(1,1))
plot(
  b0_sigma_grid, bfs_2d_b0_sigma,
  type='l',
  ylim=range(c(bfs_2d_b0_sigma, wakefield)),
  ylab = 'logBF',
  xlab = 'sigma_b0',
  col='red',
  main='Wakefield (Blue) vs 2d quad BF (Red)'
)
abline(h=wakefield, col='blue')
```

```{r}
evidence_reparam <- quad2d_reparam(x, y, 0, 1, 0, .1, n=256)
null_reparam <- quad2d_reparam(x, y, 0, 1e-10, 0, .1, n=256)
evidence_reparam - null_reparam
```

```{r}
compute_2d_bf_reparam(x, y, 1, 0.1)
compute_2d_bf_reparam(x, y, 1, 1)
compute_2d_bf_reparam(x, y, 1, 10)
compute_2d_bf_reparam(x, y, 1, 20)
```

### KL Gap Across Hyper-Parameters

An important question is if we can optimize the ELBO over hyper-parameters. In order for that to go well (ie, the results we get are similar to what we would get if we were optimizing the evidence directly) it would be reassuring to know that the gap between the ELBO and the evidence does not change drastically.

It appears that the gap is more sensitive to the intercept (hyper)parameter compared to the prior effect variance (hyper) parameter.

```{r}
library(tidyverse)

set.seed(1)
x <- rnorm(1000)
b0 <- 0
b1 <- 0.1
y <- rbinom(length(x), 1, sigmoid(x))

b0_grid <- seq(-3, 3, by=0.2)
b_sigma_grid<- seq(0.1, 4, by=0.1)

# compute quadrature integration over gaussian
quad1d_fixed_intercept <- function(x, y, b0, b_mu, b_sigma, n=128){
  # inner integral over b
  quad_b <- function(b){
    res <- conditional_likelihood(x, y, b, delta=b0, log=T)
    return(res)
  }
  
  # integrate over b0
  q <- statmod::gauss.quad.prob(n=n, dist = 'normal', mu = b_mu, sigma=b_sigma)
  integrand <- purrr::map_dbl(q$nodes, quad_b)
  res <- matrixStats::logSumExp(integrand + log(q$weights))
  return(res)
}

# compute quadrature integration over uniform
quad1d_fixed_intercept2 <- function(x, y, b0, b_mu, b_sigma, n=128, q=NULL){
  # inner integral over b
  quad_b <- function(b){
    res <- conditional_likelihood(x, y, b, b0, log=T) + dnorm(b, b_mu, b_sigma, log=T)
    return(res)
  }
  
  # integrate over b0
  q <- statmod::gauss.quad.prob(n=n, l=b_mu-10*b_sigma, u = b_mu+10*b_sigma)
  integrand <- purrr::map_dbl(q$nodes, quad_b)
  res <- matrixStats::logSumExp(integrand + log(q$weights)) + log(20*b_sigma)
  return(res)
}


fit_quad_1d_fixed_int <- function(x, y, b0, b_sigma, n=2^10){
  tictoc::tic()
  res <- quad1d_fixed_intercept2(x, y, b0, 0, b_sigma, n=n)
  tictoc::toc()
  return(res)
}

fit_grid <- cache_rds({
  tidyr::crossing(b0=b0_grid, b_sigma=b_sigma_grid) %>%
  rowwise() %>%
  mutate(vb = list(fit_univariate_vb(
    x, y, delta.init=b0, tau0=1/b_sigma^2, estimate_intercept = F))) %>%
  mutate(quad1d = fit_quad_1d_fixed_int(x, y, b0, b_sigma, n=2^10))
},file='fit_grid_1d_v_vb', hash=list(x=x, y=y))

fit_grid <- fit_grid %>% 
  mutate(elbo = tail(vb$elbos, 1), diff=quad1d-tail(vb$elbos, 1)) 

fit_grid %>% 
  ggplot(aes(x=b0, y=b_sigma, fill=exp(diff))) + 
  geom_raster() +
  scale_fill_gradient2(low='red', mid='white', high='blue', midpoint=1, limits=c(1.0, exp(max(fit_grid$diff))))

fit_grid %>% 
  filter(abs(b0) < 0.5) %>%
  ggplot(aes(x=as.numeric(b0), y=as.numeric(b_sigma), z=elbo)) + 
  geom_raster(aes(fill=elbo)) +
  scale_fill_gradient(low='darkblue', high='goldenrod2') +
  geom_contour(color='white')

fit_grid %>% 
  filter(abs(b0) < 0.5) %>%
  ggplot(aes(x=as.numeric(b0), y=as.numeric(b_sigma), z=quad1d)) + 
  geom_raster(aes(fill=quad1d)) +
  scale_fill_gradient(low='darkblue', high='goldenrod2') +
  geom_contour(color='white') 
```

###  Numerical issues with quadrauture

For some values of $\sigma_0$ the 1d quadrature has trouble... 

```{r}
bigf <- function(x, y, b0, sigma){
  f1 <- function(b_sigma){
    vb <- fit_univariate_vb(x, y, delta.init=b0, tau0=1/b_sigma^2, estimate_intercept = F)
    assertthat::assert_that(vb$monotone)
    return(tail(vb$elbos, 1))
  }
  
  f2 <- function(b_sigma){fit_quad_1d_fixed_int(x, y, b0, b_sigma, n=2^10)}
  
  f3 <- function(b_sigma){f2(b_sigma) - f1(b_sigma)}
  
  elbo <- map_dbl(sigma, f1)
  logpy <- map_dbl(sigma, f2)
  diffs <- logpy - elbo
  
  par(mfrow=c(1,3))
  plot(sigma, diffs, main='logp(y) - ELBO')
  points(sigma[diffs < 0], diffs[diffs < 0], col='red')
  plot(sigma, logpy, main='logp(y)')
  plot(sigma, elbo, main='ELBO')
}
```


```{r}
set.seed(5)
x <- rnorm(1000)
b0 <- 0
b1 <- 1
y <- rbinom(length(x), 1, sigmoid(x*b1 + b0))
bigf(x, y, 3, seq(0.01, 0.3, by=0.02))
```

```{r}
set.seed(5)
x <- rnorm(1000)
b0 <- 0
b1 <- 1
y <- rbinom(length(x), 1, sigmoid(x*b1 + b0))
bigf(x, y, 3, seq(10, 20, by=0.1))
```


